[
{
	"uri": "/docs-sat/en-22/install/",
	"title": "SAT Installation",
	"tags": [],
	"description": "",
	"content": "SAT Installation Install the System Admin Toolkit Product Stream Describes how to install the System Admin Toolkit (SAT) product stream.\nPrerequisites CSM is installed and verified. cray-product-catalog is running. There must be at least 2 gigabytes of free space on the manager NCN on which the procedure is run. Notes on the Procedures Ellipses (...) in shell output indicate omitted lines. In the examples below, replace 2.2.x with the version of the SAT product stream being installed. \u0026lsquo;manager\u0026rsquo; and \u0026lsquo;master\u0026rsquo; are used interchangeably in the steps below. To upgrade SAT, execute the pre-installation, installation, and post-installation procedures for a newer distribution. The newly installed version will become the default. Pre-Installation Procedure Start a typescript.\nThe typescript will record the commands and the output from this installation.\nncn-m001# script -af product-sat.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Installation Procedure Copy the release distribution gzipped tar file to ncn-m001.\nUnzip and extract the release distribution, 2.2.x.\nncn-m001# tar -xvzf sat-2.2.x.tar.gz Change directory to the extracted release distribution directory.\nncn-m001# cd sat-2.2.x Run the installer: install.sh.\nThe script produces a lot of output. A successful install ends with \u0026ldquo;SAT version 2.2.x has been installed\u0026rdquo;.\nncn-m001# ./install.sh ... ====\u0026gt; Updating active CFS configurations ... ====\u0026gt; SAT version 2.2.x has been installed. Upgrade only: Record the names of the CFS configuration or configurations modified by install.sh.\nThe install.sh script attempts to modify any CFS configurations that apply to the master management NCNs. During an upgrade, install.sh will log messages indicating the CFS configuration or configurations that were modified. For example, if there are three master nodes all using the same CFS configuration named \u0026ldquo;ncn-personalization\u0026rdquo;, the output would look like this:\n====\u0026gt; Updating active CFS configurations INFO: Querying CFS configurations for the following NCNs: x3000c0s1b0n0, x3000c0s3b0n0, x3000c0s5b0n0 INFO: Found configuration \u0026#34;ncn-personalization\u0026#34; for component x3000c0s1b0n0 INFO: Found configuration \u0026#34;ncn-personalization\u0026#34; for component x3000c0s3b0n0 INFO: Found configuration \u0026#34;ncn-personalization\u0026#34; for component x3000c0s5b0n0 INFO: Updating CFS configuration \u0026#34;ncn-personalization\u0026#34; INFO: Updating existing layer with repo path /vcs/cray/sat-config-management.git and playbook sat-ncn.yml in configuration \u0026#34;ncn-personalization\u0026#34;. INFO: Key \u0026#34;name\u0026#34; in layer with repo path /vcs/cray/sat-config-management.git and playbook sat-ncn.yml updated from sat-ncn to sat-2.2.16 INFO: Successfully updated layers in configuration \u0026#34;ncn-personalization\u0026#34; Save the name of each CFS configuration updated by the installer. In the previous example, a single configuration named \u0026ldquo;ncn-personalization\u0026rdquo; was updated, so that name is saved to a temporary file.\nncn-m001# echo ncn-personalization \u0026gt;\u0026gt; /tmp/sat-ncn-cfs-configurations.txt Repeat the previous command for each CFS configuration that was updated.\nUpgrade only: Save the new name of the SAT CFS configuration layer.\nIn the example install.sh output above, the new layer name is sat-2.2.16. Save this value to a file to be used later.\nncn-m001# echo sat-2.2.16 \u0026gt; /tmp/sat-layer-name.txt Fresh install only: Save the CFS configuration layer for SAT to a file for later use.\nThe install.sh script attempts to modify any CFS configurations that apply to the master management NCNs. During a fresh install, no such CFS configurations will be found, and it will instead log the SAT configuration layer that must be added to the CFS configuration that will be created. Here is an example of the output in that case:\n====\u0026gt; Updating active CFS configurations INFO: Querying CFS configurations for the following NCNs: x3000c0s1b0n0, x3000c0s3b0n0, x3000c0s5b0n0 WARNING: No CFS configurations found that apply to components with role Management and subrole Master. INFO: The following sat layer should be used in the CFS configuration that will be applied to NCNs with role Management and subrole Master. { \u0026#34;name\u0026#34;: \u0026#34;sat-2.2.15\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;9a74b8f5ba499af6fbcecfd2518a40e081312933\u0026#34;, \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/sat-config-management.git\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;sat-ncn.yml\u0026#34; } Save the JSON output to a file for later use. For example:\nncn-m001# cat \u0026gt; /tmp/sat-layer.json \u0026lt;\u0026lt;EOF \u0026gt; { \u0026gt; \u0026#34;name\u0026#34;: \u0026#34;sat-2.2.15\u0026#34;, \u0026gt; \u0026#34;commit\u0026#34;: \u0026#34;9a74b8f5ba499af6fbcecfd2518a40e081312933\u0026#34;, \u0026gt; \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/sat-config-management.git\u0026#34;, \u0026gt; \u0026#34;playbook\u0026#34;: \u0026#34;sat-ncn.yml\u0026#34; \u0026gt; } \u0026gt; EOF Do not copy the previous command verbatim. Use the JSON output from the install.sh script.\nPost-Installation Procedure Optional: Remove the SAT release distribution tar file and extracted directory.\nncn-m001# rm sat-2.2.x.tar.gz ncn-m001# rm -rf sat-2.2.x/ Upgrade only: Ensure that the environment variable SAT_TAG is not set in the ~/.bashrc file on any of the management NCNs.\nNOTE: This step should only be required when updating from Shasta 1.4.1 or Shasta 1.4.2.\nThe following example assumes three manager NCNs: ncn-m001, ncn-m002, and ncn-m003, and shows output from a system in which no further action is needed.\nncn-m001# pdsh -w ncn-m00[1-3] cat ~/.bashrc ncn-m001: source \u0026lt;(kubectl completion bash) ncn-m003: source \u0026lt;(kubectl completion bash) ncn-m002: source \u0026lt;(kubectl completion bash) The following example shows that SAT_TAG is set in ~/.bashrc on ncn-m002. Remove that line from the ~/.bashrc file on ncn-m002.\nncn-m001# pdsh -w ncn-m00[1-3] cat ~/.bashrc ncn-m001: source \u0026lt;(kubectl completion bash) ncn-m002: source \u0026lt;(kubectl completion bash) ncn-m002: export SAT_TAG=3.5.0 ncn-m003: source \u0026lt;(kubectl completion bash) Stop the typescript.\nNOTE: This step can be skipped if you wish to use the same typescript for the remainder of the SAT install. See Next Steps.\nncn-m001# exit SAT version 2.2.x is now installed/upgraded, meaning the SAT 2.2.x release has been loaded into the system software repository.\nSAT configuration content for this release has been uploaded to VCS. SAT content for this release has been uploaded to the CSM product catalog. SAT content for this release has been uploaded to Nexus repositories. The sat command won\u0026rsquo;t be available until the NCN Personalization procedure has been executed. Next Steps If other HPE Cray EX software products are being installed or upgraded in conjunction with SAT, refer to the HPE Cray EX System Software Getting Started Guide to determine which step to execute next.\nIf no other HPE Cray EX software products are being installed or upgraded at this time, proceed to the sections listed below.\nNOTE: The NCN Personalization procedure is required when upgrading SAT. The setup procedures in SAT Setup, however, are not required when upgrading SAT. They should have been executed during the first installation of SAT.\nExecute the NCN Personalization procedure:\nPerform NCN Personalization If performing a fresh install, execute the SAT Setup procedures:\nSAT Authentication Generate SAT S3 Credentials Run Sat Setrev to Set System Information If performing an upgrade, execute the upgrade procedures:\nRemove obsolete configuration file sections SAT Logging Perform NCN Personalization Describes how to perform NCN personalization using CFS. This personalization process will configure the System Admin Toolkit (SAT) product stream.\nPrerequisites The Install the System Admin Toolkit Product Stream procedure has been successfully completed. If upgrading, the names of the CFS configurations updated during installation were saved to the file /tmp/sat-ncn-cfs-configurations.txt. If upgrading, the name of the new SAT CFS configuration layer was saved to the file /tmp/sat-layer-name.txt. If performing a fresh install, the SAT CFS configuration layer was saved to the file /tmp/sat-layer.json. Notes on the Procedure Ellipses (...) in shell output indicate omitted lines. In the examples below, replace 2.2.x with the version of the SAT product stream being installed. \u0026lsquo;manager\u0026rsquo; and \u0026lsquo;master\u0026rsquo; are used interchangeably in the steps below. If upgrading SAT, the existing configuration will likely include other Cray EX product entries. Update the SAT entry as described in this procedure. The HPE Cray EX System Software Getting Started Guide provides guidance on how and when to update the entries for the other products. Procedure Start a typescript if not already using one.\nThe typescript will capture the commands and the output from this installation procedure.\nncn-m001# script -af product-sat.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Fresh install only: Add the SAT layer to the NCN personalization JSON file.\nIf the SAT install script, install.sh, did not identify and modify the CFS configurations that apply to each master management NCN, it will have printed the SAT CFS configuration layer in JSON format. This layer must be added to the JSON file being used to construct the CFS configuration. For example, if the file being used is named ncn-personalization.json, and the SAT layer was saved to the file /tmp/sat-layer.json as described in the install instructions, the following jq command will append the SAT layer and save the result in a new file named ncn-personalization.json.\nncn-m001# jq -s \u0026#39;{layers: (.[0].layers + [.[1]])}\u0026#39; ncn-personalization.json \\ /tmp/sat-layer.json \u0026gt; ncn-personalization.new.json For instructions on how to create a CFS configuration from the previous file and how to apply it to the management NCNs, refer to \u0026ldquo;Perform NCN Personalization\u0026rdquo; in the HPE Cray System Management Documentation. After the CFS configuration has been created and applied, return to this procedure.\nUpgrade only: Invoke each CFS configuration that was updated during the upgrade.\nIf the SAT install script, install.sh, identified CFS configurations that apply to the master management NCNs and modified them in place, invoke each CFS configuration that was created or updated during installation.\nThis step will create a CFS session for each given configuration and install SAT on the associated manager NCNs.\nThe --configuration-limit option limits the configuration session to run only the SAT layer of the configuration.\nYou should see a representation of the CFS session in the output.\nncn-m001# for cfs_configuration in $(cat /tmp/sat-ncn-cfs-configurations.txt); do cray cfs sessions create --name \u0026#34;sat-session-${cfs_configuration}\u0026#34; --configuration-name \\ \u0026#34;${cfs_configuration}\u0026#34; --configuration-limit $(cat /tmp/sat-layer-name.txt); done name=\u0026#34;sat-session-ncn-personalization\u0026#34; [ansible] ... Upgrade only: Monitor the progress of each CFS session.\nThis step assumes a single session named sat-session-ncn-personalization was created in the previous step.\nFirst, list all containers associated with the CFS session:\nncn-m001# kubectl get pod -n services --selector=cfsession=sat-session-ncn-personalization \\ -o json | jq \u0026#39;.items[0].spec.containers[] | .name\u0026#39; \u0026#34;inventory\u0026#34; \u0026#34;ansible-1\u0026#34; \u0026#34;istio-proxy\u0026#34; Next, get the logs for the ansible-1 container.\nNOTE: the trailing digit might differ from \u0026ldquo;1\u0026rdquo;. It is the zero-based index of the sat-ncn layer within the configuration\u0026rsquo;s layers.\nncn-m001# kubectl logs -c ansible-1 --tail 100 -f -n services \\ --selector=cfsession=sat-session-ncn-personalization Ansible plays, which are run by the CFS session, will install SAT on all the manager NCNs on the system. Successful results for all of the manager NCN xnames can be found at the end of the container log. For example:\n... PLAY RECAP ********************************************************************* x3000c0s1b0n0 : ok=3 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 x3000c0s3b0n0 : ok=3 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 x3000c0s5b0n0 : ok=3 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Execute this step for each unique CFS configuration.\nNOTE: Ensure that the PLAY RECAPs for each session show successes for all manager NCNs before proceeding.\nVerify that SAT was successfully configured.\nIf sat is configured, the --version command will indicate which version is installed. If sat is not properly configured, the command will fail.\nNOTE: This version number will differ from the version number of the SAT release distribution. This is the semantic version of the sat Python package, which is different from the version number of the overall SAT release distribution.\nncn-m001# sat --version sat 3.7.0 NOTE: Upon first running sat, you may see additional output while the sat container image is downloaded. This will occur the first time sat is run on each manager NCN. For example, if you run sat for the first time on ncn-m001 and then for the first time on ncn-m002, you will see this additional output both times.\nTrying to pull registry.local/cray/cray-sat:3.7.0-20210514024359_9fed037... Getting image source signatures Copying blob da64e8df3afc done Copying blob 0f36fd81d583 done Copying blob 12527cf455ba done ... sat 3.7.0 Stop the typescript.\nncn-m001# exit SAT version 2.2.x is now configured:\nThe SAT RPM package is installed on the associated NCNs. Next Steps If other HPE Cray EX software products are being installed or upgraded in conjunction with SAT, refer to the HPE Cray EX System Software Getting Started Guide to determine which step to execute next.\nIf no other HPE Cray EX software products are being installed or upgraded at this time, proceed to the remaining SAT Setup or SAT Post-Upgrade procedures.\nIf performing a fresh install, execute the SAT Setup procedures:\nSAT Authentication Generate SAT S3 Credentials Run Sat Setrev to Set System Information If performing an upgrade, execute the SAT Post-Upgrade procedures:\nRemove obsolete configuration file sections SAT Logging SAT Authentication Initially, as part of the installation and configuration, SAT authentication is set up so sat commands can be used in later steps of the install process. The admin account used to authenticate with sat auth must be enabled in Keycloak and must have its assigned role set to admin. For instructions on editing Role Mappings see Create Internal User Accounts in the Keycloak Shasta Realm in the CSM product documentation. For additional information on SAT authentication, see System Security and Authentication in the CSM documentation.\nNOTE: This procedure is only required after initially installing SAT. It is not required after upgrading SAT.\nDescription of SAT Command Authentication Types Some SAT subcommands make requests to the Shasta services through the API gateway and thus require authentication to the API gateway in order to function. Other SAT subcommands use the Kubernetes API. Some sat commands require S3 to be configured (see: Generate SAT S3 Credentials). In order to use the SAT S3 bucket, the System Administrator must generate the S3 access key and secret keys and write them to a local file. This must be done on every Kubernetes manager node where SAT commands are run.\nBelow is a table describing SAT commands and the types of authentication they require.\nSAT Subcommand Authentication/Credentials Required Man Page Description sat auth Responsible for authenticating to the API gateway and storing a token. sat-auth Authenticate to the API gateway and save the token. sat bmccreds Requires authentication to the API gateway. sat-bmccreds Set BMC passwords. sat bootprep Requires authentication to the API gateway. Requires kubernetes configuration and authentication, which is done on ncn-m001 during the install. sat-bootprep Prepare to boot nodes with images and configurations. sat bootsys Requires authentication to the API gateway. Requires kubernetes configuration and authentication, which is configured on ncn-m001 during the install. Some stages require passwordless SSH to be configured to all other NCNs. Requires S3 to be configured for some stages. sat-bootsys Boot or shutdown the system, including compute nodes, application nodes, and non-compute nodes (NCNs) running the management software. sat diag Requires authentication to the API gateway. sat-diag Launch diagnostics on the HSN switches and generate a report. sat firmware Requires authentication to the API gateway. sat-firmware Report firmware version. sat hwhist Requires authentication to the API gateway. sat-hwhist Report hardware component history. sat hwinv Requires authentication to the API gateway. sat-hwinv Give a listing of the hardware of the HPE Cray EX system. sat hwmatch Requires authentication to the API gateway. sat-hwmatch Report hardware mismatches. sat init None sat-init Create a default SAT configuration file. sat k8s Requires kubernetes configuration and authentication, which is automatically configured on ncn-w001 during the install. sat-k8s Report on kubernetes replicasets that have co-located replicas (i.e. replicas on the same node). sat linkhealth This command has been deprecated. sat nid2xname Requires authentication to the API gateway. sat-nid2xname Translate node IDs to node xnames. sat sensors Requires authentication to the API gateway. sat-sensors Report current sensor data. sat setrev Requires S3 to be configured for site information such as system name, serial number, install date, and site name. sat-setrev Set HPE Cray EX system revision information. sat showrev Requires API gateway authentication in order to query the Interconnect from HSM. Requires S3 to be configured for site information such as system name, serial number, install date, and site name. sat-showrev Print revision information for the HPE Cray EX system. sat slscheck Requires authentication to the API gateway. sat-slscheck Perform a cross-check between SLS and HSM. sat status Requires authentication to the API gateway. sat-status Report node status across the HPE Cray EX system. sat swap Requires authentication to the API gateway. sat-swap Prepare HSN switch or cable for replacement and bring HSN switch or cable into service. sat xname2nid Requires authentication to the API gateway. sat-xname2nid Translate node and node BMC xnames to node IDs. sat switch This command has been deprecated. It has been replaced by sat swap. In order to authenticate to the API gateway, you must run the sat auth command. This command will prompt for a password on the command line. The username value is obtained from the following locations, in order of higher precedence to lower precedence:\nThe --username global command-line option. The username option in the api_gateway section of the config file at ~/.config/sat/sat.toml. The name of currently logged in user running the sat command. If credentials are entered correctly when prompted by sat auth, a token file will be obtained and saved to ~/.config/sat/tokens. Subsequent sat commands will determine the username the same way as sat auth described above, and will use the token for that username if it has been obtained and saved by sat auth.\nPrerequisites The sat CLI has been installed following Install The System Admin Toolkit Product Stream. Procedure The following is the procedure to globally configure the username used by SAT and authenticate to the API gateway:\nGenerate a default SAT configuration file, if one does not exist.\nncn-m001# sat init Configuration file \u0026#34;/root/.config/sat/sat.toml\u0026#34; generated. Note: If the config file already exists, it will print out an error:\nERROR: Configuration file \u0026#34;/root/.config/sat/sat.toml\u0026#34; already exists. Not generating configuration file. Edit ~/.config/sat/sat.toml and set the username option in the api_gateway section of the config file. E.g.:\nusername = \u0026#34;crayadmin\u0026#34; Run sat auth. Enter your password when prompted. E.g.:\nncn-m001# sat auth Password for crayadmin: Succeeded! Other sat commands are now authenticated to make requests to the API gateway. E.g.:\nncn-m001# sat status Generate SAT S3 Credentials Generate S3 credentials and write them to a local file so the SAT user can access S3 storage. In order to use the SAT S3 bucket, the System Administrator must generate the S3 access key and secret keys and write them to a local file. This must be done on every Kubernetes master node where SAT commands are run.\nSAT uses S3 storage for several purposes, most importantly to store the site-specific information set with sat setrev (see: Run Sat Setrev to Set System Information).\nNOTE: This procedure is only required after initially installing SAT. It is not required after upgrading SAT.\nPrerequisites The SAT CLI has been installed following Install The System Admin Toolkit Product Stream The SAT configuration file has been created (See SAT Authentication). CSM has been installed and verified. Procedure Ensure the files are readable only by root.\nncn-m001# touch /root/.config/sat/s3_access_key \\ /root/.config/sat/s3_secret_key ncn-m001# chmod 600 /root/.config/sat/s3_access_key \\ /root/.config/sat/s3_secret_key Write the credentials to local files using kubectl.\nncn-m001# kubectl get secret sat-s3-credentials -o json -o \\ jsonpath=\u0026#39;{.data.access_key}\u0026#39; | base64 -d \u0026gt; \\ /root/.config/sat/s3_access_key ncn-m001# kubectl get secret sat-s3-credentials -o json -o \\ jsonpath=\u0026#39;{.data.secret_key}\u0026#39; | base64 -d \u0026gt; \\ /root/.config/sat/s3_secret_key Verify the S3 endpoint specified in the SAT configuration file is correct.\nGet the SAT configuration file\u0026rsquo;s endpoint value.\nNOTE: If the command\u0026rsquo;s output is commented out, indicated by an initial # character, the SAT configuration will take the default value – \u0026quot;https://rgw-vip.nmn\u0026quot;.\nncn-m001# grep endpoint ~/.config/sat/sat.toml # endpoint = \u0026#34;https://rgw-vip.nmn\u0026#34; Get the sat-s3-credentials secret\u0026rsquo;s endpoint value.\nncn-m001# kubectl get secret sat-s3-credentials -o json -o \\ jsonpath=\u0026#39;{.data.s3_endpoint}\u0026#39; | base64 -d | xargs https://rgw-vip.nmn Compare the two endpoint values.\nIf the values differ, change the SAT configuration file\u0026rsquo;s endpoint value to match the secret\u0026rsquo;s.\nCopy SAT configurations to each manager node on the system.\nncn-m001# for i in ncn-m002 ncn-m003; do echo $i; ssh ${i} \\ mkdir -p /root/.config/sat; \\ scp -pr /root/.config/sat ${i}:/root/.config; done NOTE: Depending on how many manager nodes are on the system, the list of manager nodes may be different. This example assumes three manager nodes, where the configuration files must be copied from ncn-m001 to ncn-m002 and ncn-m003. Therefore, the list of hosts above is ncn-m002 and ncn-m003.\nRun sat setrev to Set System Information NOTE: This procedure is only required after initially installing SAT. It is not required after upgrading SAT.\nPrerequisites S3 credentials have been generated. See Generate SAT S3 Credentials. SAT authentication has been set up. See SAT Authentication. Procedure Run sat setrev to set System Revision Information. Follow the on-screen prompts to set the following site-specific values:\nSerial number System name System type System description Product number Company name Site name Country code System install date TIP: For \u0026ldquo;System type\u0026rdquo;, a system with any liquid-cooled components should be considered a liquid-cooled system. I.e., \u0026ldquo;System type\u0026rdquo; is EX-1C.\nncn-m001# sat setrev -------------------------------------------------------------------------------- Setting: Serial number Purpose: System identification. This will affect how snapshots are identified in the HPE backend services. Description: This is the top-level serial number which uniquely identifies the system. It can be requested from an HPE representative. Valid values: Alpha-numeric string, 4 - 20 characters. Type: \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; Default: None Current value: None -------------------------------------------------------------------------------- Please do one of the following to set the value of the above setting: - Input a new value - Press CTRL-C to exit ... Run sat showrev to verify System Revision Information. The following tables contain example information.\nncn-m001# sat showrev ################################################################################ System Revision Information ################################################################################ +---------------------+---------------+ | component | data | +---------------------+---------------+ | Company name | HPE | | Country code | US | | Interconnect | Sling | | Product number | R4K98A | | Serial number | 12345 | | Site name | HPE | | Slurm version | slurm 20.02.5 | | System description | Test System | | System install date | 2021-01-29 | | System name | eniac | | System type | EX-1C | +---------------------+---------------+ ################################################################################ Product Revision Information ################################################################################ +--------------+-----------------+------------------------------+------------------------------+ | product_name | product_version | images | image_recipes | +--------------+-----------------+------------------------------+------------------------------+ | csm | 0.8.14 | cray-shasta-csm-sles15sp1... | cray-shasta-csm-sles15sp1... | | sat | 2.0.1 | - | - | | sdu | 1.0.8 | - | - | | slingshot | 0.8.0 | - | - | | sma | 1.4.12 | - | - | +--------------+-----------------+------------------------------+------------------------------+ ################################################################################ Local Host Operating System ################################################################################ +-----------+----------------------+ | component | version | +-----------+----------------------+ | Kernel | 5.3.18-24.15-default | | SLES | SLES 15-SP2 | +-----------+----------------------+ Remove obsolete configuration file sections Prerequisites The Install the System Admin Toolkit Product Stream procedure has been successfully completed. The Perform NCN Personalization procedure has been successfully completed. Procedure After upgrading SAT, if using the configuration file from a previous version, there may be configuration file sections no longer used in the new version. For example, when upgrading from Shasta 1.4 to Shasta 1.5, the [redfish] configuration file section is no longer used. In that case, the following warning may appear upon running sat commands.\nWARNING: Ignoring unknown section \u0026#39;redfish\u0026#39; in config file. Remove the [redfish] section from /root/.config/sat/sat.toml to resolve the warning.\n[redfish] username = \u0026#34;admin\u0026#34; password = \u0026#34;adminpass\u0026#34; Repeat this process for any configuration file sections for which there are \u0026ldquo;unknown section\u0026rdquo; warnings.\nSAT Logging As of SAT version 2.2, some command output that was previously printed to stdout is now logged to stderr. These messages are logged at the INFO level. The default logging threshold was changed from WARNING to INFO to accomodate this logging change. Additionally, some messages previously logged at the INFO are now logged at the DEBUG level.\nThese changes take effect automatically. However, if the default output threshold has been manually set in ~/.config/sat/sat.toml, it should be changed to ensure that important output is shown in the terminal.\nUpdate Configuration In the following example, the stderr log level, logging.stderr_level, is set to WARNING, which will exclude INFO-level logging from terminal output.\nncn-m001:~ # grep -A 3 logging ~/.config/sat/sat.toml [logging] ... stderr_level = \u0026#34;WARNING\u0026#34; To enable the new default behavior, comment this line out, delete it, or set the value to \u0026ldquo;INFO\u0026rdquo;.\nIf logging.stderr_level is commented out, its value will not affect logging behavior. However, it may be helpful set its value to INFO as a reminder of the new default behavior.\nAffected Commands The following commands trigger messages that have been changed from stdout print calls to INFO-level (or WARNING- or ERROR-level) log messages:\nsat bootsys --stage shutdown --stage session-checks sat sensors The following commands trigger messages that have been changed from INFO-level log messages to DEBUG-level log messages:\nsat nid2xname sat xname2nid sat swap Uninstall: Removing a Version of SAT Prerequisites Only versions 2.2 or newer of SAT can be uninstalled with prodmgr. Older versions must be uninstalled manually. CSM version 1.2 or newer must be installed, so that the prodmgr command is available. Procedure Use sat showrev to list versions of SAT.\nNOTE: It is not recommended to uninstall a version designated as \u0026ldquo;active\u0026rdquo;. If the active version is uninstalled, then the activate procedure must be executed on a remaining version.\nncn-m001# sat showrev --products --filter product_name=sat ############################################################################### Product Revision Information ############################################################################### +--------------+-----------------+--------+-------------------+-----------------------+ | product_name | product_version | active | images | image_recipes | +--------------+-----------------+--------+-------------------+-----------------------+ | sat | 2.3.3 | True | - | - | | sat | 2.2.10 | False | - | - | +--------------+-----------------+--------+-------------------+-----------------------+ Use prodmgr to uninstall a version of SAT.\nThis command will do three things:\nRemove all hosted-type package repositories associated with the given version of SAT. Group-type repositories are not removed. Remove all container images associated with the given version of SAT. Remove SAT from the cray-product-catalog Kubernetes ConfigMap, so that it will no longer show up in the output of sat showrev. ncn-m001# prodmgr uninstall sat 2.2.10 Repository sat-2.2.10-sle-15sp2 has been removed. Removed Docker image cray/cray-sat:3.9.0 Removed Docker image cray/sat-cfs-install:1.0.2 Removed Docker image cray/sat-install-utility:1.4.0 Deleted sat-2.2.10 from product catalog. Activate: Switching Between Versions This procedure can be used to downgrade the active version of SAT.\nPrerequisites Only versions 2.2 or newer of SAT can be activated. Older versions must be activated manually. CSM version 1.2 or newer must be installed, so that the prodmgr command is available. Procedure Use sat showrev to list versions of SAT.\nncn-m001# sat showrev --products --filter product_name=sat ############################################################################### Product Revision Information ############################################################################### +--------------+-----------------+--------+--------------------+-----------------------+ | product_name | product_version | active | images | image_recipes | +--------------+-----------------+--------+--------------------+-----------------------+ | sat | 2.3.3 | True | - | - | | sat | 2.2.10 | False | - | - | +--------------+-----------------+--------+--------------------+-----------------------+ Use prodmgr to activate a different version of SAT.\nThis command will do three things:\nFor all hosted-type package repositories associated with this version of SAT, set them as the sole member of their corresponding group-type repository. For example, activating SAT version 2.2.10 sets the repository sat-2.2.10-sle-15sp2 as the only member of the sat-sle-15sp2 group. Set the version 2.2.10 as active within the product catalog, so that it appears active in the output of sat showrev. Ensure that the SAT CFS configuration content exists as a layer in all CFS configurations that are associated with NCNs with the role \u0026ldquo;Management\u0026rdquo; and subrole \u0026ldquo;Master\u0026rdquo; (for example, the CFS configuration ncn-personalization). Specifically, it will ensure that the layer refers to the version of SAT CFS configuration content associated with the version of SAT being activated. ncn-m001# prodmgr activate sat 2.2.10 Repository sat-2.2.10-sle-15sp2 is now the default in sat-sle-15sp2. Set sat-2.2.10 as active in product catalog. Updated CFS configurations: [ncn-personalization] Verify that the chosen version is marked as active.\nncn-m001# sat showrev --products --filter product_name=sat ############################################################################### Product Revision Information ############################################################################### +--------------+-----------------+--------+--------------------+-----------------------+ | product_name | product_version | active | images | image_recipes | +--------------+-----------------+--------+--------------------+-----------------------+ | sat | 2.3.3 | False | - | - | | sat | 2.2.10 | True | - | - | +--------------+-----------------+--------+--------------------+-----------------------+ Run NCN Personalization.\nAt this point, the command has modified Nexus package repositories to set a particular package repository as active, but no packages on the NCNs have been changed. In order to complete the activation process, NCN Personalization must be executed to change the cray-sat-podman package version on the manager NCNs.\nNOTE: Refer to the command output from step 2 for the names of all CFS configurations that were updated, which may not necessarily be just ncn-personalization. If multiple configurations were updated in step 2, then a cray cfs sessions create command should be run for each of them. This example assumes a single configuration named ncn-personalization was updated. If multiple were updated, set cfs_configurations to a space-separated list below.\nncn-m001# cfs_configurations=\u0026#34;ncn-personalization\u0026#34; ncn-m001# for cfs_configuration in ${cfs_configurations} do cray cfs sessions create --name \u0026#34;sat-session-${cfs_configuration}\u0026#34; --configuration-name \\ \u0026#34;${cfs_configuration}\u0026#34; --configuration-limit sat-ncn; done Monitor the progress of each CFS session.\nThis step assumes a single session named sat-session-ncn-personalization was created in the previous step.\nFirst, list all containers associated with the CFS session:\nncn-m001# kubectl get pod -n services --selector=cfsession=sat-session-ncn-personalization \\ -o json | jq \u0026#39;.items[0].spec.containers[] | .name\u0026#39; \u0026#34;inventory\u0026#34; \u0026#34;ansible-1\u0026#34; \u0026#34;istio-proxy\u0026#34; Next, get the logs for the ansible-1 container.\nNOTE: the trailing digit might differ from \u0026ldquo;1\u0026rdquo;. It is the zero-based index of the sat-ncn layer within the configuration\u0026rsquo;s layers.\nncn-m001# kubectl logs -c ansible-1 --tail 100 -f -n services \\ --selector=cfsession=sat-session-ncn-personalization Ansible plays, which are run by the CFS session, will install SAT on all the manager NCNs on the system. Successful results for all of the manager NCN xnames can be found at the end of the container log. For example:\n... PLAY RECAP ********************************************************************* x3000c0s1b0n0 : ok=3 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 x3000c0s3b0n0 : ok=3 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 x3000c0s5b0n0 : ok=3 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Execute this step for each unique CFS configuration.\nNOTE: Ensure that the PLAY RECAPs for each session show successes for all manager NCNs before proceeding.\nVerify the new version of the SAT CLI.\nNOTE: This version number will differ from the version number of the SAT release distribution. This is the semantic version of the SAT Python package, which is different from the version number of the overall SAT release distribution.\nncn-m001# sat --version 3.9.0 "
},
{
	"uri": "/docs-sat/en-22/",
	"title": "HPE Cray EX System Admin Toolkit (SAT) Guide",
	"tags": [],
	"description": "",
	"content": "HPE Cray EX System Admin Toolkit (SAT) Guide Introduction to SAT About System Admin Toolkit (SAT) System Admin Toolkit Command Overview Command Prompt Conventions in SAT SAT Dependencies SAT Installation Install SAT Install the System Admin Toolkit Product Stream Perform NCN Personalization SAT Setup SAT Authentication Generate SAT S3 Credentials Run sat setrev to Set System Information SAT Post-Upgrade Remove obsolete configuration file sections SAT Logging SAT Uninstall and Downgrade Uninstall: Removing a Version of SAT Activate: Switching Between Versions SAT Dashboards SAT Kibana Dashboards SAT Grafana Dashboards SAT Usage SAT Bootprep SAT Release Notes Summary of changes in SAT 2.2 Summary of SAT changes in Shasta v1.5 Summary of SAT Changes in Shasta v1.4.1 Summary of SAT Changes in Shasta v1.4 Summary of SAT Changes in Shasta v1.3.2 Summary of SAT Changes in Shasta v1.3 "
},
{
	"uri": "/docs-sat/en-22/dashboards/",
	"title": "SAT Dashboards",
	"tags": [],
	"description": "",
	"content": "SAT Dashboards SAT Kibana Dashboards SAT Grafana Dashboards "
},
{
	"uri": "/docs-sat/en-22/introduction/",
	"title": "Introduction to SAT",
	"tags": [],
	"description": "",
	"content": "Introduction to SAT About System Admin Toolkit (SAT) The System Admin Toolkit (SAT) is designed to assist administrators with common tasks, such as troubleshooting and querying information about the HPE Cray EX System and its components, system boot and shutdown, and replacing hardware components.\nSAT offers a command line utility which uses subcommands. There are similarities between SAT commands and xt commands used on the Cray XC platform. For more information on SAT commands, see System Admin Toolkit Command Overview.\nSix Kibana Dashboards are included with SAT. They provide organized output for system health information.\nAER Kibana Dashboard ATOM Kibana Dashboard Heartbeat Kibana Dashboard Kernel Kibana Dashboard MCE Kibana Dashboard Rasdaemon Kibana Dashboard Four Grafana Dashboards are included with SAT. They display messages that are generated by the HSN (High Speed Network) and are reported through Redfish.\nGrafana Fabric Congestion Dashboard Grafana Fabric Errors Dashboard Grafana Fabric Port State Dashboard Grafana Fabric RFC3635 Dashboard SAT is installed as a separate product as part of the HPE Cray EX System base installation.\nSystem Admin Toolkit Command Overview Describes the SAT Command Line Utility, lists the key commands found in the System Admin Toolkit man pages, and provides instruction on the SAT Container Environment.\nSAT Command Line Utility The primary component of the System Admin Toolkit (SAT) is a command-line utility run from Kubernetes manager nodes (ncn-m nodes).\nIt is designed to assist administrators with common tasks, such as troubleshooting and querying information about the HPE Cray EX System and its components, system boot and shutdown, and replacing hardware components. There are similarities between SAT commands and xt commands used on the Cray XC platform.\nSAT Commands The top-level SAT man page describes the toolkit, documents the global options affecting all subcommands, documents configuration file options, and references the man page for each subcommand. SAT consists of many subcommands that each have their own set of options.\nSAT Container Environment The sat command-line utility runs in a container using podman, a daemonless container runtime. SAT runs on Kubernetes manager nodes. A few important points about the SAT container environment include the following:\nUsing either sat or sat bash always launches a container. The SAT container does not have access to the NCN file system. There are two ways to run sat.\nInteractive: Launching a container using sat bash, followed by a sat command. Non-interactive: Running a sat command directly on a Kubernetes manager node. In both of these cases, a container is launched in the background to execute the command. The first option, running sat bash first, gives an interactive shell, at which point sat commands can be run. In the second option, the container is launched, executes the command, and upon the command\u0026rsquo;s completion the container exits. The following two examples show the same action, checking the system status, using interactive and non-interactive modes.\nInteractive ncn-m001# sat bash (CONTAINER-ID)sat-container# sat status Non-interactive ncn-m001# sat status Interactive Advantages Running sat using the interactive command prompt gives the ability to read and write local files on ephemeral container storage. If multiple sat commands are being run in succession, then use sat bash to launch the container beforehand. This will save time because the container does not need to be launched for each sat command.\nNon-interactive Advantages The non-interactive mode is useful if calling sat with a script, or when running a single sat command as a part of several steps that need to be executed from a management NCN.\nMan Pages - Interactive and Non-interactive Modes To view a sat man page from a Kubernetes manager node, use sat-man on the manager node as shown in the following example.\nncn-m001# sat-man status A man page describing the SAT container environment is available on the Kubernetes manager nodes, which can be viewed either with man sat or man sat-podman from the manager node.\nncn-m001# man sat ncn-m001# man sat-podman Command Prompt Conventions in SAT The host name in a command prompt indicates where the command must be run. The account that must run the command is also indicated in the prompt.\nThe root or super-user account always has the # character at the end of the prompt and has the host name of the host in the prompt. Any non-root account is indicated with account@hostname\u0026gt;. A user account that is neither root nor crayadm is referred to as user. The command prompt inside the SAT container environment is indicated with the string as follows. It also has the \u0026ldquo;#\u0026rdquo; character at the end of the prompt. Command Prompt Meaning ncn-m001# Run on one of the Kubernetes Manager servers. (Non-interactive) (CONTAINER_ID) sat-container# Run the command inside the SAT container environment by first running sat bash. (Interactive) Examples of the sat status command used by an administrator:\nncn-m001# sat status ncn-m001# sat bash (CONTAINER_ID) sat-container# sat status SAT Dependencies Most sat subcommands depend on services or components from other products in the HPE Cray EX (Shasta) software stack. The following list shows these dependencies for each subcommand. Each service or component is listed under the product it belongs to.\nsat auth CSM Keycloak sat bmccreds CSM System Configuration Service (SCSD) sat bootprep CSM Boot Orchestration Service (BOS) Configuration Framework Service (CFS) Image Management Service (IMS) Version Control Service (VCS) Kubernetes S3 sat bootsys CSM Boot Orchestration Service (BOS) Cray Advanced Platform Monitoring and Control (CAPMC) Ceph Compute Rolling Upgrade Service (CRUS) Etcd Firmware Action Service (FAS) Hardware State Manager (HSM) Kubernetes S3 COS Node Memory Dump (NMD) sat diag CSM Hardware State Manager (HSM) CSM-Diag Fox sat firmware CSM Firmware Action Service (FAS) sat hwhist CSM Hardware State Manager (HSM) sat hwinv CSM Hardware State Manager (HSM) sat hwmatch CSM Hardware State Manager (HSM) sat init None\nsat k8s CSM Kubernetes sat nid2xname CSM Hardware State Manager (HSM) sat sensors CSM Hardware State Manager (HSM) HM Collector SMA Telemetry API sat setrev CSM S3 sat showrev CSM Hardware State Manager (HSM) Kubernetes S3 sat slscheck CSM Hardware State Manager (HSM) Kubernetes S3 sat status CSM Hardware State Manager (HSM) sat swap Slingshot Fabric Manager sat switch Deprecated: See sat swap\nsat xname2nid CSM Hardware State Manager (HSM) "
},
{
	"uri": "/docs-sat/en-22/dashboards/sat_grafana_dashboards/",
	"title": "SAT Grafana Dashboards",
	"tags": [],
	"description": "",
	"content": "SAT Grafana Dashboards The SAT Grafana Dashboards display messages that are generated by the HSN (High Speed Network) and reported through Redfish. The messages are displayed based on severity.\nGrafana can be accessed via web browser at the following URL:\nhttps://sma-grafana.\u0026lt;site-domain\u0026gt; The value of site-domain can be obtained as follows:\nncn-m001:~ # kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | \\ base64 -d | grep \u0026#34;external:\u0026#34; That command will produce the following output, for example:\nexternal: EXAMPLE_DOMAIN.com This would result in the address for Grafana being https://sma-grafana.EXAMPLE_DOMAIN.com\nFor additional details about how to access the Grafana Dashboards refer to Access the Grafana Monitoring UI in the SMA product documentation.\nFor more information about the interpretation of metrics for the SAT Grafana Dashboards refer to Fabric Telemetry Kafka Topics in the SMA product documentation.\nNavigate SAT Grafana Dashboards There are four Fabric Telemetry dashboards used in SAT that report on the HSN. Two contain chart panels and two display telemetry in a tabular format.\nDashboard Name Display Type Fabric Congestion Chart Panels Fabric RFC3635 Chart Panels Fabric Errors Tabular Format Fabric Port State Tabular Format The tabular format presents a single point of telemetry for a given location and metric, either because the telemetry is not numerical or that it changes infrequently. The value shown is the most recently reported value for that location during the time range selected, if any. The interval setting is not used for tabular dashboards.\nSAT Grafana Interval and Locations Options Shows the Interval and Locations Options for the available telemetry.\nThe value of the Interval option sets the time resolution of the received telemetry. This works a bit like a histogram, with the available telemetry in an interval of time going into a \u0026ldquo;bucket\u0026rdquo; and averaging out to a single point on the chart or table. The special value auto will choose an interval based on the time range selected.\nFor additional information, refer to Grafana Templates and Variables.\nThe Locations option allows restriction of the telemetry shown by locations, either individual links or all links in a switch. The selection presented updates dynamically according to time range, except for the errors dashboard, which always has entries for all links and switches, although the errors shown are restricted to the selected time range.\nThe chart panels for the RFC3635 and Congestion dashboards allow selection of a single location from the chart\u0026rsquo;s legend or the trace on the chart.\nGrafana Fabric Congestion Dashboard SAT Grafana Dashboards provide system administrators a way to view fabric telemetry data across all Rosetta switches in the system and assess the past and present health of the high-speed network. It also allows the ability to drill down to view data for specific ports on specific switches.\nThis dashboard contains the variable, Port Type not found in the other dashboards. The possible values are edge, local, and global and correspond to the link\u0026rsquo;s relationship to the network topology. The locations presented in the panels are restricted to the values (any combination, defaults to \u0026ldquo;all\u0026rdquo;) selected.\nThe metric values for links of a given port type are similar in value to each other but very distinct from the values of other types. If the values for different port types are all plotted together, the values for links with lower values are indistinguishable from zero when plotted.\nThe port type of a link is reported as a port state \u0026ldquo;subtype\u0026rdquo; event when defined at port initialization.\nGrafana Fabric Errors Dashboard This dashboard reports error counters in a tabular format in three panels.\nThere is no Interval option because this parameter is not used to set a coarseness of the data. Only a single value is presented that displays the most recent value in the time range.\nUnlike other dashboards, the locations presented are all locations in the system rather than having telemetry within the time range selected. However, the values are taken from telemetry within the time range.\nGrafana Fabric Port State Dashboard There is no Interval option because this parameter is not used to set a coarseness of the data. Only a single value is presented that displays the most recent value in the time range.\nThe Fabric Port State telemetry is distinct because it typically is not numeric. It also updates infrequently, so a long time range may be necessary to obtain any values. Port State is refreshed daily, so a time range of 24 hours results in all states for all links in the system being shown.\nThe three columns named, group, switch, and port are not port state events, but extra information included with all port state events.\nGrafana Fabric RFC3635 Dashboard For additional information on performance counters, refer to Definitions of Managed Objects for the Ethernet-like Interface Types, an Internet standards document.\nBecause these metrics are counters that only increase over time, the values plotted are the change in the counter\u0026rsquo;s value over the interval setting.\n"
},
{
	"uri": "/docs-sat/en-22/dashboards/sat_kibana_dashboards/",
	"title": "SAT Kibana Dashboards",
	"tags": [],
	"description": "",
	"content": "SAT Kibana Dashboards Kibana is an open source analytics and visualization platform designed to search, view, and interact with data stored in Elasticsearch indices. Kibana runs as a web service and has a browser-based interface. It offers visual output of node data in the forms of charts, tables and maps that display real-time Elasticsearch queries. Viewing system data in this way breaks down the complexity of large data volumes into easily understood information.\nKibana can be accessed via web browser at the following URL:\nhttps://sma-kibana.\u0026lt;site-domain\u0026gt; The value of site-domain can be obtained as follows:\nncn-m001:~ # kubectl get secret site-init -n loftsman -o jsonpath=\u0026#39;{.data.customizations\\.yaml}\u0026#39; | \\ base64 -d | grep \u0026#34;external:\u0026#34; That command will produce the following output, for example:\nexternal: EXAMPLE_DOMAIN.com This would result in the address for Kibana being https://sma-kibana.EXAMPLE_DOMAIN.com\nFor additional details about how to access the Kibana Dashboards refer to View Logs Via Kibana in the SMA product documentation.\nAdditional details about the AER, ATOM, Heartbeat, Kernel, MCE, and Rasdaemon Kibana Dashboards are included in this table.\nDashboard Short Description Long Description Kibana Visualization and Search Name sat-aer AER corrected Corrected Advanced Error Reporting messages from PCI Express devices on each node. Visualization: aer-corrected Search: sat-aer-corrected sat-aer AER fatal Fatal Advanced Error Reporting messages from PCI Express devices on each node. Visualization: aer-fatal Search: sat-aer-fatal sat-atom ATOM failures Application Task Orchestration and Management tests are run on a node when a job finishes. Test failures are logged. sat-atom-failed sat-atom ATOM admindown Application Task Orchestration and Management test failures can result in nodes being marked admindown. An admindown node is not available for job launch. sat-atom-admindown sat-heartbeat Heartbeat loss events Heartbeat loss event messages reported by the hbtd pods that monitor for heartbeats across nodes in the system. sat-heartbeat sat-kernel Kernel assertions The kernel software performs a failed assertion when some condition represents a serious fault. The node goes down. sat-kassertions sat-kernel Kernel panics The kernel panics when something is seriously wrong. The node goes down. sat-kernel-panic sat-kernel Lustre bugs (LBUGs) The Lustre software in the kernel stack performs a failed assertion when some condition related to file system logic represents a serious fault. The node goes down. sat-lbug sat-kernel CPU stalls CPU stalls are serous conditions that can reduce node performance, and sometimes cause a node to go down. Technically these are Read-Copy-Update stalls where software in the kernel stack holds onto memory for too long. Read-Copy-Update is a vital aspect of kernel performance and rather esoteric. sat-cpu-stall sat-kernel Out of memory An Out Of Memory (OOM) condition has occurred. The kernel must kill a process to continue. The kernel will select an expendable process when possible. If there is no expendable process the node usually goes down in some manner. Even if there are expendable processes the job is likely to be impacted. OOM conditions are best avoided. sat-oom sat-mce MCE Machine Check Exceptions (MCE) are errors detected at the processor level. sat-mce sat-rasdaemon rasdaemon errors Errors from the rasdaemon service on nodes. The rasdaemon service is the Reliability, Availability, and Serviceability Daemon, and it is intended to collect all hardware error events reported by the linux kernel, including PCI and MCE errors. This may include certain HSN errors in the future. sat-rasdaemon-error sat-rasdaemon rasdaemon messages All messages from the rasdaemon service on nodes. sat-rasdaemon Disable Search Highlighting in Kibana Dashboard By default, search highlighting is enabled. This procedure instructs how to disable search highlighting.\nThe Kibana Dashboard should be open on your system.\nNavigate to Management\nNavigate to Advanced Settings in the Kibana section, below the Elastic search section\nScroll down to the Discover section\nChange Highlight results from on to off\nClick Save to save changes\nAER Kibana Dashboard The AER Dashboard displays errors that come from the PCI Express Advanced Error Reporting (AER) driver. These errors are split up into separate visualizations depending on whether they are fatal or corrected errors.\nView the AER Kibana Dashboard Go to the dashboard section.\nSelect sat-aer dashboard.\nChoose the time range of interest.\nView the Corrected and Fatal Advanced Error Reporting messages from PCI Express devices on each node. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\nATOM Kibana Dashboard The ATOM (Application Task Orchestration and Management) Dashboard displays node failures that occur during health checks and application test failures. Some test failures are of possible interest even though a node is not marked admindown or otherwise fails. They are of clear interest if a node is marked admindown, and might provide clues if a node otherwise fails. They might also show application problems.\nView the ATOM Kibana Dashboard HPE Cray EX is installed on the system along with the System Admin Toolkit, which contains the ATOM Kibana Dashboard.\nGo to the dashboard section.\nSelect sat-atom dashboard.\nChoose the time range of interest.\nView any nodes marked admindown and any ATOM test failures. These failures occur during health checks and application test failures. Test failures marked admindown are important to note. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\nHeartbeat Kibana Dashboard The Heartbeat Dashboard displays heartbeat loss messages that are logged by the hbtd pods in the system. The hbtd pods are responsible for monitoring nodes in the system for heartbeat loss.\nView the Heartbeat Kibana Dashboard Go to the dashboard section.\nSelect sat-heartbeat dashboard.\nChoose the time range of interest.\nView the heartbeat loss messages that are logged by the hbtd pods in the system. The hbtd pods are responsible for monitoring nodes in the system for heartbeat loss.View the matching log messages in the panel.\nKernel Kibana Dashboard The Kernel Dashboard displays compute node failures such as kernel assertions, kernel panics, and Lustre LBUG messages. The messages reveal if Lustre has experienced a fatal error on any compute nodes in the system. A CPU stall is a serious problem that might result in a node failure. Out-of-memory conditions can be due to applications or system problems and may require expert analysis. They provide useful clues for some node failures and may reveal if an application is using too much memory.\nView the Kernel Kibana Dashboard Go to the dashboard section.\nSelect sat-kernel dashboard.\nChoose the time range of interest.\nView the compute node failures such as kernel assertions, kernel panics, and Lustre LBUG messages. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\nMCE Kibana Dashboard The MCE Dashboard displays CPU detected processor-level hardware errors.\nView the MCE Kibana Dashboard Go to the dashboard section.\nSelect sat-mce dashboard.\nChoose the time range of interest.\nView the Machine Check Exceptions (MCEs) listed including the counts per NID (node). For an MCE, the CPU number and DIMM number can be found in the message, if applicable. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\nRasdaemon Kibana Dashboard The Rasdaemon Dashboard displays errors that come from the Reliability, Availability, and Serviceability (RAS) daemon service on nodes in the system. This service collects all hardware error events reported by the linux kernel, including PCI and MCE errors. As a result there may be some duplication between the messages presented here and the messages presented in the MCE and AER dashboards. This dashboard splits up the messages into two separate visualizations, one for only messages of severity \u0026ldquo;emerg\u0026rdquo; or \u0026ldquo;err\u0026rdquo; and another for all messages from rasdaemon.\nView the Rasdaemon Kibana Dashboard Go to the dashboard section.\nSelect sat-rasdaemon dashboard.\nChoose the time range of interest.\nView the errors that come from the Reliability, Availability, and Serviceability (RAS) daemon service on nodes in the system. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\n"
},
{
	"uri": "/docs-sat/en-22/release_notes/",
	"title": "SAT Release Notes",
	"tags": [],
	"description": "",
	"content": "SAT Release Notes Summary of changes in SAT 2.2 SAT 2.2.16 was released on February 25th, 2022.\nThis version of the SAT product included:\nVersion 3.14.0 of the sat python package and CLI Version 1.6.4 of the sat-podman wrapper script Version 1.0.4 of the sat-cfs-install container image and Helm chart It also added the following new components:\nVersion 1.4.3 of the sat-install-utility container image Version 2.0.2 of the cfs-config-util container image The following sections detail the changes in this release.\nKnown issues in SAT 2.2 sat command unavailable in sat bash shell After launching a shell within the SAT container with sat bash, the sat command will not be found. For example:\n(CONTAINER-ID) sat-container:~ # sat status bash: sat: command not found This can be resolved temporarily in one of two ways. /sat/venv/bin/ may be prepended to the $PATH environment variable:\n(CONTAINER-ID) sat-container:~ # export PATH=/sat/venv/bin:$PATH (CONTAINER-ID) sat-container:~ # sat status Or, the file /sat/venv/bin/activate may be sourced:\n(CONTAINER-ID) sat-container:~ # source /sat/venv/bin/activate (CONTAINER-ID) sat-container:~ # sat status Tab completion unavailable in sat bash shell After launching a shell within the SAT container with sat bash, tab completion for sat commands does not work.\nThis can be resolved temporarily by sourcing the file /etc/bash_completion.d/sat-completion.bash:\nsource /etc/bash_completion.d/sat-completion.bash OCI runtime permission error when running sat in root directory sat commands will not work if the current directory is /. For example:\nncn-m001:/ # sat --help Error: container_linux.go:380: starting container process caused: process_linux.go:545: container init caused: open /dev/console: operation not permitted: OCI runtime permission denied error To resolve, run sat in another directory.\nDuplicate mount error when running sat in config directory sat commands will not work if the current directory is ~/.config/sat. For example:\nncn-m001:~/.config/sat # sat --help Error: /root/.config/sat: duplicate mount destination To resolve, run sat in another directory.\nNew sat commands sat bootprep automates the creation of CFS configurations, the build and customization of IMS images, and the creation of BOS session templates. See SAT Bootprep for details. sat slscheck performs a check for consistency between the System Layout Service (SLS) and the Hardware State Manager (HSM). sat bmccreds provides a simple interface for interacting with the System Configuration Service (SCSD) to set BMC Redfish credentials. sat hwhist displays hardware component history by xname (location) or by its Field-Replaceable Unit ID (FRUID). This command queries the Hardware State Manager (HSM) API to obtain this information. Since the sat hwhist command supports querying for the history of a component by its FRUID, the FRUID of components has been added to the output of sat hwinv. Additional Install Automation The following automation has been added to the install script, install.sh:\nWait for the completion of the sat-config-import Kubernetes job, which is started when the sat-cfs-install Helm chart is deployed. Automate the modification of the CFS configuration, which applies to master management NCNs (e.g. \u0026ldquo;ncn-personalization\u0026rdquo;). Changes to Product Catalog Data Schema The SAT product uploads additional information to the cray-product-catalog Kubernetes ConfigMap detailing the components it provides, including container (Docker) images, Helm charts, RPMs, and package repositories.\nThis information is used to support uninstall and activation of SAT product versions moving forward.\nSupport for Uninstall and Activation of SAT Versions Beginning with the 2.2 release, SAT now provides partial support for the uninstall and activation of the SAT product stream.\nSee Uninstall: Removing a Version of SAT and Activate: Switching Between Versions for details.\nImprovements to sat status A Subrole column has been added to the output of sat status. This allows you to easily differentiate between master, worker, and storage nodes in the management role, for example.\nHostname information from SLS has been added to sat status output.\nAdded Support for JSON Output Support for JSON-formatted output has been added to commands which currently support the --format option, such as hwinv, status, and showrev.\nUsability Improvements Many usability improvements have been made to multiple sat commands, mostly related to filtering command output. The following are some highlights:\nAdded --fields option to display only specific fields for subcommands which display tabular reports. Added ability to filter on exact matches of a field name. Improved handling of multiple matches of a field name in --filter queries so that the first match is used, similar to --sort-by. Added support for --filter, --fields, and --reverse for summaries displayed by sat hwinv. Added borders to summary tables generated by sat hwinv. Improved documentation in the man pages. Default Log Level Changed The default log level for stderr has been changed from \u0026ldquo;WARNING\u0026rdquo; to \u0026ldquo;INFO\u0026rdquo;. For details, see SAT Logging.\nMore Granular Log Level Configuration Options With the command-line options --loglevel-stderr and --loglevel-file, the log level can now be configured separately for stderr and the log file.\nThe existing --loglevel option is now an alias for the --loglevel-stderr option.\nPodman Wrapper Script Improvements The Podman wrapper script is the script installed at /usr/bin/sat on the master management NCNs by the cray-sat-podman RPM that runs the cray-sat container in podman. The following subsections detail improvements that were made to the wrapper script in this release.\nMounting of $HOME and Current Directories in cray-sat Container The Podman wrapper script that launches the cray-sat container with podman has been modified to mount the user\u0026rsquo;s current directory and home directory into the cray-sat container to provide access to local files in the container.\nPodman Wrapper Script Documentation Improvements The man page for the Podman wrapper script, which is accessed by typing man sat on a master management NCN, has been improved to document the following:\nEnvironment variables that affect execution of the wrapper script Host files and directories mounted in the container Fixes to Podman Wrapper Script Output Redirection Fixed issues with redirecting stdout and stderr, and piping output to commands, such as awk, less, and more.\nConfigurable HTTP Timeout A new sat option has been added to configure the HTTP timeout length for requests to the API gateway. See sat-man sat for details.\nsat bootsys Improvements Many improvements and fixes have been made to sat bootsys. The following are some highlights:\nAdded the --excluded-ncns option, which can be used to omit NCNs from the platform-services and ncn-power stages in case they are inaccessible. Disruptive shutdown stages in sat bootsys shutdown now prompt the user to continue before proceeding. A new option, --disruptive, will bypass this. Improvements to Ceph service health checks and restart during the platform-services stage of sat bootsys boot. sat xname2nid Improvements sat xname2nid can now recursively expand slot, chassis, and cabinet xnames to a list of nids in those locations.\nA new --format option has been added to sat xname2nid. It sets the output format to either \u0026ldquo;range\u0026rdquo; (the default) or \u0026ldquo;nid\u0026rdquo;. The \u0026ldquo;range\u0026rdquo; format displays nids in a compressed range format suitable for use with a workload manager like Slurm.\nUsage of v2 HSM API The commands which interact with HSM (e.g., sat status and sat hwinv) now use the v2 HSM API.\nsat diag Limited to HSN Switches sat diag will now only operate against HSN switches by default. These are the only controllers that support running diagnostics with HMJTD.\nsat showrev Enhancements A column has been added to the output of sat showrev that indicates whether a product version is \u0026ldquo;active\u0026rdquo;. The definition of \u0026ldquo;active\u0026rdquo; varies across products, and not all products may set an \u0026ldquo;active\u0026rdquo; version.\nFor SAT, the active version is the one with its hosted-type package repository in Nexus set as the member of the group-type package repository in Nexus, meaning that it will be used when installing the cray-sat-podman RPM.\ncray-sat Container Image Size Reduction The size of the cray-sat container image has been approximately cut in half by leveraging multi-stage builds. This also improved the repeatability of the unit tests by running them in the container.\nBug Fixes Minor bug fixes were made in cray-sat and in cray-sat-podman. For full change lists, see each repository\u0026rsquo;s CHANGELOG.md file.\nSummary of SAT changes in Shasta v1.5 We released version 2.1.16 of the SAT product in Shasta v1.5.\nThis version of the SAT product included:\nVersion 3.7.4 of the sat python package and CLI Version 1.4.10 of the sat-podman wrapper script It also added the following new component:\nVersion 1.0.3 of the sat-cfs-install docker image and helm chart The following sections detail the changes in this release.\nInstall Changes to Separate Product from CSM This release further decouples the installation of the SAT product from the CSM product. The cray-sat-podman RPM is no longer installed in the management non-compute node (NCN) image. Instead, the cray-sat-podman RPM is installed on all master management NCNs via an Ansible playbook which is referenced by a layer of the CFS configuration that applies to management NCNs. This CFS configuration is typically named \u0026ldquo;ncn-personalization\u0026rdquo;.\nThe SAT product now includes a Docker image and a Helm chart named sat-cfs-install. The SAT install script, install.sh, deploys the Helm chart with Loftsman. This helm chart deploys a Kubernetes job that imports the SAT Ansible content to a git repository in VCS (Gitea) named sat-config-management. This repository is referenced by the layer added to the NCN personalization CFS configuration.\nRemoval of Direct Redfish Access All commands which used to access Redfish directly have either been removed or modified to use higher-level service APIs. This includes the following commands:\nsat sensors sat diag sat linkhealth The sat sensors command has been rewritten to use the SMA telemetry API to obtain the latest sensor values. The command\u0026rsquo;s usage has changed slightly, but legacy options work as before, so it is backwards compatible. Additionally, new commands have been added.\nThe sat diag command has been rewritten to use a new service called Fox, which is delivered with the CSM-diags product. The sat diag command now launches diagnostics using the Fox service, which launches the corresponding diagnostic executables on controllers using the Hardware Management Job and Task Daemon (HMJTD) over Redfish. Essentially, Fox serves as a proxy for us to start diagnostics over Redfish.\nThe sat linkhealth command has been removed. Its functionality has been replaced by functionality from the Slingshot Topology Tool (STT) in the fabric manager pod.\nThe Redfish username and password command line options and config file options have been removed. For further instructions, see Remove Obsolete Configuration File Sections.\nAdditional Fields in sat setrev and sat showrev sat setrev now collects the following information from the admin, which is then displayed by sat showrev:\nSystem description Product number Company name Country code Additional guidance and validation has been added to each field collected by sat setrev. This sets the stage for sdu setup to stop collecting this information and instead collect it from sat showrev or its S3 bucket.\nImprovements to sat bootsys The platform-services stage of the sat bootsys boot command has been improved to start inactive Ceph services, unfreeze Ceph, and wait for Ceph health in the correct order. The ceph-check stage has been removed as it is no longer needed.\nThe platform-services stage of sat bootsys boot now prompts for confirmation of the storage NCN hostnames in addition to the Kubernetes masters and workers.\nBug Fixes and Security Fixes Improved error handling in sat firmware. Incremented version of Alpine Linux to 3.13.2 to address a security vulnerability. Other Notable Changes Ansible has been removed from the cray-sat container image. Support for the Firmware Update Service (FUS) has been removed from the sat firmware command. Summary of SAT Changes in Shasta v1.4.1 We released version 2.0.4 of the SAT product in Shasta v1.4.1.\nThis version of the SAT product included:\nVersion 3.5.0 of the sat python package and CLI. Version 1.4.3 of the sat-podman wrapper script. The following sections detail the changes in this release.\nNew Commands to Translate Between NIDs and XNames Two new commands were added to translate between NIDs and XNames:\nsat nid2xname sat xname2nid These commands perform this translation by making requests to the Hardware State Manager (HSM) API.\nBug Fixes Fixed a problem in sat swap where creating the offline port policy failed. Changed sat bootsys shutdown --stage bos-operations to no longer forcefully power off all compute nodes and application nodes using CAPMC when BOS sessions complete or time out. Fixed an issue with the command sat bootsys boot --stage cabinet-power. Summary of SAT Changes in Shasta v1.4 In Shasta v1.4, SAT became an independent product, which meant we began to designate a version number for the entire SAT product. We released version 2.0.3 of the SAT product in Shasta v1.4.\nThis version of the SAT product included the following components:\nVersion 3.4.0 of the sat python package and CLI It also added the following new component:\nVersion 1.4.2 of the sat-podman wrapper script The following sections detail the changes in this release.\nSAT as an Independent Product SAT is now packaged and released as an independent product. The product deliverable is called a \u0026ldquo;release distribution\u0026rdquo;. The release distribution is a gzipped tar file containing an install script. This install script loads the cray/cray-sat container image into the Docker registry in Nexus and loads the cray-sat-podman RPM into a package repository in Nexus.\nIn this release, the cray-sat-podman package is still installed in the master and worker NCN images provided by CSM. This is changed in SAT 2.1.16 released in Shasta v1.5.\nSAT Running in a Container Under Podman The sat command now runs in a container under Podman. The sat executable is now installed on all nodes in the Kubernetes management cluster (i.e., workers and masters). This executable is a wrapper script that starts a SAT container in Podman and invokes the sat Python CLI within that container. The admin can run individual sat commands directly on the master or worker NCNs as before, or they can run sat commands inside the SAT container after using sat bash to enter an interactive shell inside the SAT container.\nTo view man pages for sat commands, the user can run sat-man SAT_COMMAND, replacing SAT_COMMAND with the name of the sat command. Alternatively, the user can enter the sat container with sat bash and use the man command.\nNew sat init Command and Config File Location Change The default location of the SAT config file has been changed from /etc/sat.toml to ~/.config/sat/sat.toml. A new command, sat init, has been added that initializes a configuration file in the new default directory. This better supports individual users on the system who want their own config files.\n~/.config/sat is mounted into the container that runs under Podman, so changes are persistent across invocations of the sat container. If desired, an alternate configuration directory can be specified with the SAT_CONFIG_DIR environment variable.\nAdditionally, if a config file does not yet exist when a user runs a sat command, one is generated automatically.\nAdditional Types Added to sat hwinv Additional functionality has been added to sat hwinv including:\nList node enclosure power supplies with the --list-node-enclosure-power-supplies option. List node accelerators (e.g., GPUs) with the --list-node-accels option. The count of node accelerators is also included for each node. List node accelerator risers (e.g., Redstone modules) with the --list-node-accel-risers option. The count of node accelerator risers is also included for each node. List High-Speed Node Network Interface Cards (HSN NICs) with the --list-node-hsn-nics option. The count of HSN NICs is also included for each node. Documentation for these new options has been added to the man page for sat hwinv.\nSite Information Stored by sat setrev in S3 The sat setrev and sat showrev commands now use S3 to store and obtain site information, including system name, site name, serial number, install date, and system type. Since the information is stored in S3, it will now be consistent regardless of the node on which sat is executed.\nAs a result of this change, S3 credentials must be configured for SAT. For detailed instructions, see Generate SAT S3 Credentials.\nProduct Version Information Shown by sat showrev sat showrev now shows product information from the cray-product-catalog ConfigMap in Kubernetes.\nAdditional Changes to sat showrev The output from sat showrev has also been changed in the following ways:\nThe --docker and --packages options were considered misleading and have been removed. Information pertaining to only to the local host, where the command is run, has been moved to the output of the --local option. Removal of sat cablecheck The sat cablecheck command has been removed. To verify that the system\u0026rsquo;s Slingshot network is cabled correctly, admins should now use the show cables command in the Slingshot Topology Tool (STT).\nsat swap Command Compatibility with Next-gen Fabric Controller The sat swap command was added in Shasta v1.3.2. This command used the Fabric Controller API. Shasta v1.4 introduced a new Fabric Manager API and removed the Fabric Controller API, so this command has been rewritten to use the new backwards-incompatible API. Usage of the command did not change.\nsat bootsys Functionality Much of the functionality added to sat bootsys in Shasta v1.3.2 was broken by changes introduced in Shasta v1.4, which removed the Ansible inventory and playbooks.\nThe functionality in the platform-services stage of sat bootsys has been re-implemented to use python directly instead of Ansible. This resulted in a more robust procedure with better logging to the sat log file. Failures to stop containers on Kubernetes nodes are handled more gracefully, and more information about the containers that failed to stop, including how to debug the problem, is included.\nImprovements were made to console logging setup for non-compute nodes (NCNs) when they are shut down and booted.\nThe following improvements were made to the bos-operations stage of sat bootsys:\nMore information about the BOS sessions, BOA jobs, and BOA pods is printed. A command-line option, --bos-templates, and a corresponding config-file option, bos_templates, were added, and the --cle-bos-template and --uan-bos-template options and their corresponding config file options were deprecated. The following functionality has been removed from sat bootsys:\nThe hsn-bringup stage of sat bootsys boot has been removed due to removal of the underlying Ansible playbook. The bgp-check stage of sat bootys {boot,shutdown} has been removed. It is now a manual procedure. Log File Location Change The location of the sat log file has changed from /var/log/cray/sat.log to /var/log/cray/sat/sat.log. This change simplifies mounting this file into the sat container running under Podman.\nSummary of SAT Changes in Shasta v1.3.2 Shasta v1.3.2 included version 2.4.0 of the sat python package and CLI.\nThe following sections detail the changes in this release.\nsat swap Command for Switch and Cable Replacement The sat switch command which supported operations for replacing a switch has been deprecated and replaced with the sat swap command, which now supports replacing a switch OR cable.\nThe sat swap switch command is equivalent to sat switch. The sat switch command will be removed in a future release.\nAddition of Stages to sat bootsys Command The sat bootsys command now has multiple stages for both the boot and shutdown actions. Please refer to the \u0026ldquo;System Power On Procedures\u0026rdquo; and \u0026ldquo;System Power Off Procedures\u0026rdquo; sections of the Cray Shasta Administration Guide (S-8001) for more details on using this command in the context of a full system power off and power on.\nSummary of SAT Changes in Shasta v1.3 Shasta v1.3 included version 2.2.3 of the sat python package and CLI.\nThis version of the sat CLI contained the following commands:\nauth bootsys cablecheck diag firmware hwinv hwmatch k8s linkhealth sensors setrev showrev status swap switch See the System Admin Toolkit Command Overview and the table of commands in the SAT Authentication section of this document for more details on each of these commands.\n"
},
{
	"uri": "/docs-sat/en-22/usage/",
	"title": "SAT Usage",
	"tags": [],
	"description": "",
	"content": "SAT Usage SAT Bootprep SAT provides an automated solution for creating CFS configurations, building and configuring images in IMS, and creating BOS session templates based on a given input file which defines how those configurations, images, and session templates should be created.\nThis automated process centers around the sat bootprep command. Man page documentation for sat bootprep can be viewed similarly to other SAT commands.\nncn-m001# sat-man sat-bootprep SAT Bootprep vs SAT Bootsys sat bootprep is used to create CFS configurations, build and rename IMS images, and create BOS session templates which tie the configurations and images together during a BOS session.\nsat bootsys automates several portions of the boot and shutdown processes, including (but not limited to) performing BOS operations (such as creating BOS sessions), powering on and off cabinets, and checking the state of the system prior to shutdown.\nEditing a bootprep input file The input file provided to sat bootprep is a YAML-formatted file containing information which CFS, IMS, and BOS use to create configurations, images, and BOS session templates respectively. Writing and modifying these input files is the main task associated with using sat bootprep. An input file is composed of three main sections, one each for configurations, images, and session templates. These sections may be specified in any order, and any of the sections may be omitted if desired.\nCreating CFS configurations The configurations section begins with a configurations: key.\n--- configurations: Under this key, the user can list one or more configurations to create. For each configuration, a name should be given, in addition to the list of layers which comprise the configuration. Each layer can be defined by a product name and optionally a version number, or commit hash or branch in the product\u0026rsquo;s configuration repository. Alternatively, a layer can be defined by a Git repository URL directly, along with an associated branch or commit hash.\nWhen a configuration layer is specified in terms of a product name, the layer is created in CFS by looking up relevant configuration information (including the configuration repository and commit information) from the cray-product-catalog Kubernetes ConfigMap as necessary. A version may be supplied, but if it is absent, the version is assumed to be the latest version found in the cray-product-catalog.\n--- configurations: - name: example-configuration layers: - name: example product playbook: example.yml product: name: example version: 1.2.3 Alternatively, a configuration layer may be specified by explicitly referencing the desired configuration repository, along with the branch containing the intended version of the Ansible playbooks. A commit hash may be specified by replacing branch with commit.\n... - name: another example product playbook: another-example.yml git: url: \u0026#34;https://vcs.local/vcs/another-example-config-management.git\u0026#34; branch: main ... When sat bootprep is run against an input file, a CFS configuration will be created corresponding to each configuration in the configurations section. For example, the configuration created from an input file with the layers listed above might look something like the following:\n{ \u0026#34;lastUpdated\u0026#34;: \u0026#34;2022-02-07T21:47:49Z\u0026#34;, \u0026#34;layers\u0026#34;: [ { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://vcs.local/vcs/example-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;commit hash\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;example product\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;example.yml\u0026#34; }, { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://vcs.local/vcs/another-example-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;\u0026lt;commit hash\u0026gt;\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;another example product\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;another-example.yml\u0026#34; } ], \u0026#34;name\u0026#34;: \u0026#34;example-configuration\u0026#34; } Creating IMS images After specifying configurations, the user may add images to the input file which are to be built by IMS. To add an images section, the user should add an images key.\n--- configurations: ... (omitted for brevity) images: Under the images key, the user may define one or more images to be created in a list. Each element of the list defines a separate IMS image to be built and/or configured. Images must contain a name, as well as an ims section containing a definition of the image to be built and/or configured. Images may be defined by an image recipe, or by a pre-built image. Recipes and pre-built images are referred to by their names or IDs in IMS. The ims section should also contain an is_recipe property, which indicates whether the name or ID refers to an image recipe or a pre-built image. Images may also optionally provide a text description of the image. This description is not stored or used by sat bootprep or any CSM services, but is useful for documenting images in the input file.\n--- configurations: ... (omitted for brevity) images: - name: example-compute-image description: \u0026gt; An example compute node image for illustrative purposes. ims: name: example-compute-image-recipe is_recipe: true - name: another-example-compute-image description: \u0026gt; Another example compute node image. ims: id: \u0026lt;IMS image UUID\u0026gt; is_recipe: false Images may also contain a configuration property in their definition, which specifies a configuration with which to customize the built image prior to booting. If a configuration is specified, then configuration groups must also be specified using the configuration_group_names property.\n--- configurations: ... (omitted for brevity) images: - name: example-compute-image description: \u0026gt; An example compute node image for illustrative purposes. ims: name: example-compute-image-recipe is_recipe: true configuration: example configuration configuration_group_names: - Compute Creating BOS session templates BOS session templates are the final section of the input file, and are defined under the session_templates key.\n--- configurations: ... (omitted for brevity) images: ... (omitted for brevity) session_templates: Each session template is defined in terms of its name, an image, a configuration, and a set of parameters which can be used to configure the session. The name, image, and configuration are specified with their respective name, image, and configuration keys. bos_parameters may also be specified; currently, the only setting under bos_parameters that is supported is boot_sets, which can be used to define boot sets in the BOS session template. Each boot set is defined under its own property under boot_sets, and the value of each boot set can contain the following properties, all of which are optional:\nkernel_parameters: the parameters passed to the kernel on the command line network: the network over which the nodes will boot node_list: nodes to add to the boot set node_roles_groups: HSM roles to add to the boot set node_groups: HSM groups to add to the boot set rootfs_provider: the root file system provider rootfs_provider_passthrough: parameters to add to the rootfs= kernel parameter The properties listed previously are the same as the parameters that can be specified directly through BOS boot sets. More information can be found in the CSM documentation on session templates. Additional properties not listed are passed through to the BOS session template as written.\nAn example session template might look like the following:\nconfigurations: ... (omitted for brevity) images: ... (omitted for brevity) session_templates: - name: example-session-template image: example-image configuration: example-configuration bos_parameters: boot_sets: example_boot_set: kernel_parameters: ip=dhcp quiet node_list: [] rootfs_provider: cpss3 rootfs_provider_passthrough: dvs:api-gw-service-nmn.local:300:nmn0 Example bootprep input files Putting together all of the previous input file sections, an example bootprep input file might look something like the following.\n--- configurations: - name: cos-config layers: - name: cos-integration-2.2.87 playbook: site.yml product: name: cos version: 2.2.87 branch: integration - name: cpe-integration-21.12.3 playbook: pe_deploy.yml product: name: cpe version: 21.12.3 branch: integration - name: slurm-master-1.1.1 playbook: site.yml product: name: slurm version: 1.1.1 branch: master images: - name: cray-shasta-compute-sles15sp3.x86_64-2.2.35 ims: is_recipe: true name: cray-shasta-compute-sles15sp3.x86_64-2.2.35 configuration: cos-config configuration_group_names: - Compute session_templates: - name: cray-shasta-compute-sles15sp3.x86_64-2.2.35 image: cray-shasta-compute-sles15sp3.x86_64-2.2.35 configuration: cos-config bos_parameters: boot_sets: compute: kernel_parameters: ip=dhcp quiet spire_join_token=${SPIRE_JOIN_TOKEN} node_roles_groups: - Compute Creating a pre-populated example bootprep input file It is possible to create an example bootprep input file using values from the system\u0026rsquo;s product catalog using the sat bootprep generate-example command.\nncn-m001# sat bootprep generate-example INFO: Using latest version (2.3.24-20220113160653) of product cos INFO: Using latest version (21.11.4) of product cpe INFO: Using latest version (1.0.7) of product slurm INFO: Using latest version (1.1.24) of product analytics INFO: Using latest version (2.1.5) of product uan INFO: Using latest version (21.11.4) of product cpe INFO: Using latest version (1.0.7) of product slurm INFO: Using latest version (1.1.24) of product analytics INFO: Using latest version (2.3.24-20220113160653) of product cos INFO: Using latest version (2.1.5) of product uan INFO: Wrote example bootprep input file to ./example-bootprep-input.yaml. This file should be reviewed and edited to match the desired parameters of the configurations, images, and session templates.\nViewing built-in generated documentation The contents of the YAML input files described above must conform to a schema which defines the structure of the data. The schema definition is written using the JSON Schema format. (Although the format is named \u0026ldquo;JSON Schema\u0026rdquo;, the schema itself is written in YAML as well.) More information, including introductory materials and a formal specification of the JSON Schema metaschema, can be found on the JSON Schema website.\nViewing the exact schema specification To view the exact schema specification, run sat bootprep view-schema.\nncn-m001# sat bootprep view-schema --- $schema: \u0026#34;https://json-schema.org/draft-07/schema\u0026#34; title: Bootprep Input File description: \u0026gt; A description of the set of CFS configurations to create, the set of IMS images to create and optionally customize with the defined CFS configurations, and the set of BOS session templates to create that reference the defined images and configurations. type: object additionalProperties: false properties: ... Generating user-friendly documentation The raw schema definition can be difficult to understand without experience working with JSON Schema specifications. For this reason, a feature was included which can generate user-friendly HTML documentation for the input file schema which can be browsed with the user\u0026rsquo;s preferred web browser.\nCreate a documentation tarball using sat bootprep.\nncn-m001# sat bootprep generate-docs INFO: Wrote input schema documentation to /root/bootprep-schema-docs.tar.gz An alternate output directory can be specified with the --output-dir option. The generated tarball is always named bootprep-schema-docs.tar.gz.\nncn-m001# sat bootprep generate-docs --output-dir /tmp INFO: Wrote input schema documentation to /tmp/bootprep-schema-docs.tar.gz From another machine, copy the tarball to a local directory.\nanother-machine$ scp root@ncn-m001:bootprep-schema-docs.tar.gz . Extract the contents of the tarball and open the contained index.html.\nanother-machine$ tar xzvf bootprep-schema-docs.tar.gz x bootprep-schema-docs/ x bootprep-schema-docs/index.html x bootprep-schema-docs/schema_doc.css x bootprep-schema-docs/schema_doc.min.js another-machine$ open bootprep-schema-docs/index.html "
},
{
	"uri": "/docs-sat/en-22/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs-sat/en-22/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]