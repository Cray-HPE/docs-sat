[
{
	"uri": "/docs-sat/en-21/",
	"title": "HPE Cray EX System Admin Toolkit (SAT) Guide",
	"tags": [],
	"description": "",
	"content": "HPE Cray EX System Admin Toolkit (SAT) Guide Introduction to SAT About System Admin Toolkit (SAT) System Admin Toolkit Command Overview Command Prompt Conventions in SAT SAT Dependencies SAT Installation Install SAT Install the System Admin Toolkit Product Stream Perform NCN Personalization SAT Setup SAT Authentication Generate SAT S3 Credentials Run sat setrev to Set System Information SAT Post-Upgrade Optional: Remove old versions after an upgrade Remove obsolete configuration file sections SAT Dashboards SAT Kibana Dashboards SAT Grafana Dashboards "
},
{
	"uri": "/docs-sat/en-21/dashboards/",
	"title": "SAT Dashboards",
	"tags": [],
	"description": "",
	"content": "SAT Dashboards SAT Kibana Dashboards SAT Grafana Dashboards "
},
{
	"uri": "/docs-sat/en-21/install/",
	"title": "SAT Installation",
	"tags": [],
	"description": "",
	"content": "SAT Installation Install the System Admin Toolkit Product Stream Describes how to install the System Admin Toolkit (SAT) product stream.\nPrerequisites CSM is installed and verified. cray-product-catalog is running. There must be at least 2 gigabytes of free space on the manager NCN on which the procedure is run. Notes on the Procedures Ellipses (...) in shell output indicate omitted lines. In the examples below, replace 2.1.x with the version of the SAT product stream being installed. \u0026lsquo;manager\u0026rsquo; and \u0026lsquo;master\u0026rsquo; are used interchangeably in the steps below. To upgrade SAT, execute the pre-installation, installation, and post-installation procedures for a newer distribution. The newly installed version will become the default. Pre-Installation Procedure Start a typescript.\nThe typescript will record the commands and the output from this installation.\nncn-m001# script -af product-sat.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Installation Procedure Copy the release distribution gzipped tar file to ncn-m001.\nUnzip and extract the release distribution, 2.1.x.\nncn-m001# tar -xvzf sat-2.1.x.tar.gz Change directory to the extracted release distribution directory.\nncn-m001# cd sat-2.1.x Run the installer: install.sh.\nThe script produces a lot of output. The last several lines are included below for reference.\nncn-m001# ./install.sh ... ConfigMap data updates exist; Exiting. + clean-install-deps + for image in \u0026#34;${vendor_images[@]}\u0026#34; + podman rmi -f docker.io/library/cray-nexus-setup:sat-2.1.x-20210804163905-8dbb87d Untagged: docker.io/library/cray-nexus-setup:sat-2.1.x-20210804163905-8dbb87d Deleted: 2c196c0c6364d9a1699d83dc98550880dc491cc3433a015d35f6cab1987dd6da + for image in \u0026#34;${vendor_images[@]}\u0026#34; + podman rmi -f docker.io/library/skopeo:sat-2.1.x-20210804163905-8dbb87d Untagged: docker.io/library/skopeo:sat-2.1.x-20210804163905-8dbb87d Deleted: 1b38b7600f146503e246e753cd9df801e18409a176b3dbb07b0564e6bc27144c Check the return code of the installer. Zero indicates a successful installation.\nncn-m001# echo $? 0 Check the progress of the SAT configuration import Kubernetes job, which is initiated by install.sh.\nIf the \u0026ldquo;Pods Statuses\u0026rdquo; appear as \u0026ldquo;Succeeded\u0026rdquo;, the job has completed successfully. The job usually takes between 30 seconds and 2 minutes.\nncn-m001# kubectl describe job sat-config-import-2.1.x -n services ... Pods Statuses: 0 Running / 1 Succeeded / 0 Failed ... The job\u0026rsquo;s progress may be monitored using kubectl logs. The example below includes the final log lines from a successful configuration import Kubernetes job.\nncn-m001# kubectl logs -f -n services --selector \\ job-name=sat-config-import-2.1.x --all-containers ... ConfigMap update attempt=1 Resting 1s before reading ConfigMap ConfigMap data updates exist; Exiting. 2021-08-04T21:50:10.275886Z info Agent has successfully terminated 2021-08-04T21:50:10.276118Z warning envoy main caught SIGTERM # Completed on Wed Aug 4 21:49:44 2021 The following error may appear in this log, but it can be ignored.\nerror accept tcp [::]:15020: use of closed network connection Post-Installation Procedure Optional: Remove the SAT release distribution tar file and extracted directory.\nncn-m001# rm sat-2.2.x.tar.gz ncn-m001# rm -rf sat-2.2.x/ Upgrade only: Ensure that the environment variable SAT_TAG is not set in the ~/.bashrc file on any of the management NCNs.\nNOTE: This step should only be required when updating from Shasta 1.4.1 or Shasta 1.4.2.\nThe following example assumes three manager NCNs: ncn-m001, ncn-m002, and ncn-m003, and shows output from a system in which no further action is needed.\nncn-m001# pdsh -w ncn-m00[1-3] cat ~/.bashrc ncn-m001: source \u0026lt;(kubectl completion bash) ncn-m003: source \u0026lt;(kubectl completion bash) ncn-m002: source \u0026lt;(kubectl completion bash) The following example shows that SAT_TAG is set in ~/.bashrc on ncn-m002. Remove that line from the ~/.bashrc file on ncn-m002.\nncn-m001# pdsh -w ncn-m00[1-3] cat ~/.bashrc ncn-m001: source \u0026lt;(kubectl completion bash) ncn-m002: source \u0026lt;(kubectl completion bash) ncn-m002: export SAT_TAG=3.5.0 ncn-m003: source \u0026lt;(kubectl completion bash) Stop the typescript.\nNOTE: This step can be skipped if you wish to use the same typescript for the remainder of the SAT install. See Next Steps.\nncn-m001# exit SAT version 2.1.x is now installed/upgraded, meaning the SAT 2.1.x release has been loaded into the system software repository.\nSAT configuration content for this release has been uploaded to VCS. SAT content for this release has been uploaded to the CSM product catalog. SAT content for this release has been uploaded to Nexus repositories. The sat command won\u0026rsquo;t be available until the NCN Personalization procedure has been executed. Next Steps If other HPE Cray EX software products are being installed or upgraded in conjunction with SAT, refer to the HPE Cray EX System Software Getting Started Guide to determine which step to execute next.\nIf no other HPE Cray EX software products are being installed or upgraded at this time, proceed to the sections listed below.\nNOTE: The NCN Personalization procedure is required when upgrading SAT. The setup procedures in SAT Setup, however, are not required when upgrading SAT. They should have been executed during the first installation of SAT.\nExecute the NCN Personalization procedure:\nPerform NCN Personalization If performing a fresh install, execute the SAT Setup procedures:\nSAT Authentication Generate SAT S3 Credentials Run Sat Setrev to Set System Information If performing an upgrade, execute the upgrade procedures:\nOptional: Remove old versions after an upgrade Remove obsolete configuration file sections Perform NCN Personalization Describes how to perform NCN personalization using CFS. This personalization process will configure the System Admin Toolkit (SAT) product stream.\nPrerequisites The Install the System Admin Toolkit Product Stream procedure has been successfully completed. Notes on the Procedure Ellipses (...) in shell output indicate omitted lines. In the examples below, replace 2.1.x with the version of the SAT product stream being installed. \u0026lsquo;manager\u0026rsquo; and \u0026lsquo;master\u0026rsquo; are used interchangeably in the steps below. If upgrading SAT, the existing configuration will likely include other Cray EX product entries. Update the SAT entry as described in this procedure. The HPE Cray EX System Software Getting Started Guide provides guidance on how and when to update the entries for the other products. Procedure Start a typescript if not already using one.\nThe typescript will capture the commands and the output from this installation procedure.\nncn-m001# script -af product-sat.$(date +%Y-%m-%d).txt ncn-m001# export PS1=\u0026#39;\\u@\\H \\D{%Y-%m-%d} \\t \\w # \u0026#39; Get the git commit ID for the branch with a version number matching the version of SAT.\nThis represents a revision of Ansible configuration content stored in VCS.\nGet and store the VCS password (required to access the remote VCS repo).\nncn-m001# VCS_PASS=$(kubectl get secret -n services vcs-user-credentials \\ --template={{.data.vcs_password}} | base64 --decode) In this example, the git commit ID is 82537e59c24dd5607d5f5d6f92cdff971bd9c615, and the version number is 2.1.x.\nncn-m001# git ls-remote \\ https://crayvcs:$VCS_PASS@api-gw-service-nmn.local/vcs/cray/sat-config-management.git \\ refs/heads/cray/sat/* ... 82537e59c24dd5607d5f5d6f92cdff971bd9c615 refs/heads/cray/sat/2.1.x Add a sat layer to the CFS configuration(s) associated with the manager NCNs.\nGet the name(s) of the CFS configuration(s).\nNOTE: Each manager NCN uses a single CFS configuration. An individual CFS configuration may be used by any number of manage NCNs, i.e., three manager NCNs might use one, two, or three CFS configurations.\nIn the following example, all three manager NCNs use the same CFS configuration â€“ ncn-personalization.\nncn-m001:~ # for component in $(cray hsm state components list \\ --role Management --subrole Master --format json | jq -r \\ \u0026#39;.Components | .[].ID\u0026#39;); do cray cfs components describe $component \\ --format json | jq -r \u0026#39;.desiredConfig\u0026#39;; done ncn-personalization ncn-personalization ncn-personalization In the following example, the three manager NCNs all use different configurations, each with a unique name.\nncn-personalization-m001 ncn-personalization-m002 ncn-personalization-m003 Execute the following sub-steps (3.2 through 3.5) once for each unique CFS configuration name.\nNOTE: Examples in the following sub-steps assume that all manager NCNs use the CFS configuration ncn-personalization.\nGet the current configuration layers for each CFS configuration, and save the data to a local JSON file.\nThe JSON file created in this sub-step will serve as a template for updating an existing CFS configuration, or creating a new one.\nncn-m001# cray cfs configurations describe ncn-personalization --format \\ json | jq \u0026#39;{ layers }\u0026#39; \u0026gt; ncn-personalization.json If the configuration does not exist yet, you may see the following error. In this case, create a new JSON file for that CFS configuration, e.g., ncn-personalization.json.\nError: Configuration could not found.: Configuration ncn-personalization could not be found NOTE: For more on CFS configuration management, refer to \u0026ldquo;Manage a Configuration with CFS\u0026rdquo; in the CSM product documentation.\nAppend a sat layer to the end of the JSON file\u0026rsquo;s list of layers.\nIf the file already contains a sat layer entry, update it.\nIf the configuration data could not be found in the previous sub-step, the JSON file will be empty. In this case, copy the ncn-personalization.json example below, paste it into the JSON file, delete the ellipsis, and make appropriate changes to the sat layer entry.\nUse the git commit ID from step 8, e.g. 82537e59c24dd5607d5f5d6f92cdff971bd9c615.\nNOTE: The name value in the example below may be changed, but the installation procedure uses the example value, sat-ncn. If an alternate value is used, some of the following examples must be updated accordingly before they are executed.\nncn-m001# vim ncn-personalization.json ... ncn-m001# cat ncn-personalization.json { \u0026#34;layers\u0026#34;: [ ... { \u0026#34;cloneUrl\u0026#34;: \u0026#34;https://api-gw-service-nmn.local/vcs/cray/sat-config-management.git\u0026#34;, \u0026#34;commit\u0026#34;: \u0026#34;82537e59c24dd5607d5f5d6f92cdff971bd9c615\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;sat-ncn\u0026#34;, \u0026#34;playbook\u0026#34;: \u0026#34;sat-ncn.yml\u0026#34; } ] } Update the existing CFS configuration, or create a new one.\nThe command should output a JSON-formatted representation of the CFS configuration, which will look like the JSON file, but with lastUpdated and name fields.\nncn-m001# cray cfs configurations update ncn-personalization --file \\ ncn-personalization.json --format json { \u0026#34;lastUpdated\u0026#34;: \u0026#34;2021-08-05T16:38:53Z\u0026#34;, \u0026#34;layers\u0026#34;: { ... }, \u0026#34;name\u0026#34;: \u0026#34;ncn-personalization\u0026#34; } Optional: Delete the JSON file.\nNOTE: There is no reason to keep the file. If you keep it, verify that it is up-to-date with the actual CFS configuration before using it again.\nncn-m001# rm ncn-personalization.json Invoke the CFS configurations that you created or updated in the previous step.\nThis step will create a CFS session based on the given configuration and install SAT on the associated manager NCNs.\nThe --configuration-limit option causes only the sat-ncn layer of the configuration, ncn-personalization, to run.\nCAUTION: In this example, the session --name is sat-session. That value is only an example. Declare a unique name for each configuration session.\nYou should see a representation of the CFS session in the output.\nncn-m001# cray cfs sessions create --name sat-session --configuration-name \\ ncn-personalization --configuration-limit sat-ncn name=\u0026#34;sat-session\u0026#34; [ansible] ... Execute this step once for each unique CFS configuration that you created or updated in the previous step.\nMonitor the progress of each CFS session.\nFirst, list all containers associated with the CFS session:\nncn-m001# kubectl get pod -n services --selector=cfsession=sat-session \\ -o json | jq \u0026#39;.items[0].spec.containers[] | .name\u0026#39; \u0026#34;inventory\u0026#34; \u0026#34;ansible-1\u0026#34; \u0026#34;istio-proxy\u0026#34; Next, get the logs for the ansible-1 container.\nNOTE: the trailing digit might differ from \u0026ldquo;1\u0026rdquo;. It is the zero-based index of the sat-ncn layer within the configuration\u0026rsquo;s layers.\nncn-m001# kubectl logs -c ansible-1 --tail 100 -f -n services \\ --selector=cfsession=sat-session Ansible plays, which are run by the CFS session, will install SAT on all the manager NCNs on the system. Successful results for all of the manager NCN xnames can be found at the end of the container log. For example:\n... PLAY RECAP ********************************************************************* x3000c0s1b0n0 : ok=3 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 x3000c0s3b0n0 : ok=3 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 x3000c0s5b0n0 : ok=3 changed=3 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0 Execute this step for each unique CFS configuration.\nNOTE: Ensure that the PLAY RECAPs for each session show successes for all manager NCNs before proceeding.\nVerify that SAT was successfully configured.\nIf sat is configured, the --version command will indicate which version is installed. If sat is not properly configured, the command will fail.\nNOTE: This version number will differ from the version number of the SAT release distribution. This is the semantic version of the sat Python package, which is different from the version number of the overall SAT release distribution.\nncn-m001# sat --version sat 3.7.0 NOTE: Upon first running sat, you may see additional output while the sat container image is downloaded. This will occur the first time sat is run on each manager NCN. For example, if you run sat for the first time on ncn-m001 and then for the first time on ncn-m002, you will see this additional output both times.\nTrying to pull registry.local/cray/cray-sat:3.7.0-20210514024359_9fed037... Getting image source signatures Copying blob da64e8df3afc done Copying blob 0f36fd81d583 done Copying blob 12527cf455ba done ... sat 3.7.0 Stop the typescript.\nncn-m001# exit SAT version 2.1.x is now configured:\nThe SAT RPM package is installed on the associated NCNs. Next Steps If other HPE Cray EX software products are being installed or upgraded in conjunction with SAT, refer to the HPE Cray EX System Software Getting Started Guide to determine which step to execute next.\nIf no other HPE Cray EX software products are being installed or upgraded at this time, proceed to the remaining SAT Setup or SAT Post-Upgrade procedures.\nIf performing a fresh install, execute the SAT Setup procedures:\nSAT Authentication Generate SAT S3 Credentials Run Sat Setrev to Set System Information If performing an upgrade, execute the SAT Post-Upgrade procedures:\nOptional: Remove old versions after an upgrade Remove obsolete configuration file sections SAT Authentication Initially, as part of the installation and configuration, SAT authentication is set up so sat commands can be used in later steps of the install process. The admin account used to authenticate with sat auth must be enabled in Keycloak and must have its assigned role set to admin. For instructions on editing Role Mappings see Create Internal User Accounts in the Keycloak Shasta Realm in the CSM product documentation. For additional information on SAT authentication, see System Security and Authentication in the CSM documentation.\nNOTE: This procedure is only required after initially installing SAT. It is not required after upgrading SAT.\nDescription of SAT Command Authentication Types Some SAT subcommands make requests to the Shasta services through the API gateway and thus require authentication to the API gateway in order to function. Other SAT subcommands use the Kubernetes API. Some sat commands require S3 to be configured (see: Generate SAT S3 Credentials). In order to use the SAT S3 bucket, the System Administrator must generate the S3 access key and secret keys and write them to a local file. This must be done on every Kubernetes manager node where SAT commands are run.\nBelow is a table describing SAT commands and the types of authentication they require.\nSAT Subcommand Authentication/Credentials Required Man Page Description sat auth Responsible for authenticating to the API gateway and storing a token. sat-auth Authenticate to the API gateway and save the token. sat bootsys Requires authentication to the API gateway. Requires kubernetes configuration and authentication, which is configured on ncn-m001 during the install. Some stages require passwordless SSH to be configured to all other NCNs. Requires S3 to be configured for some stages. sat-bootsys Boot or shutdown the system, including compute nodes, application nodes, and non-compute nodes (NCNs) running the management software. sat diag Requires authentication to the API gateway. sat-diag Launch diagnostics on the HSN switches and generate a report. sat firmware Requires authentication to the API gateway. sat-firmware Report firmware version. sat hwinv Requires authentication to the API gateway. sat-hwinv Give a listing of the hardware of the HPE Cray EX system. sat hwmatch Requires authentication to the API gateway. sat-hwmatch Report hardware mismatches. sat init None sat-init Create a default SAT configuration file. sat k8s Requires kubernetes configuration and authentication, which is automatically configured on ncn-w001 during the install. sat-k8s Report on kubernetes replicasets that have co-located replicas (i.e. replicas on the same node). sat linkhealth This command has been deprecated. sat nid2xname Requires authentication to the API gateway. sat-nid2xname Translate node IDs to node xnames. sat sensors Requires authentication to the API gateway. sat-sensors Report current sensor data. sat setrev Requires S3 to be configured for site information such as system name, serial number, install date, and site name. sat-setrev Set HPE Cray EX system revision information. sat showrev Requires API gateway authentication in order to query the Interconnect from HSM. Requires S3 to be configured for site information such as system name, serial number, install date, and site name. sat-showrev Print revision information for the HPE Cray EX system. sat status Requires authentication to the API gateway. sat-status Report node status across the HPE Cray EX system. sat swap Requires authentication to the API gateway. sat-swap Prepare HSN switch or cable for replacement and bring HSN switch or cable into service. sat xname2nid Requires authentication to the API gateway. sat-xname2nid Translate node and node BMC xnames to node IDs. sat switch This command has been deprecated. It has been replaced by sat swap. In order to authenticate to the API gateway, you must run the sat auth command. This command will prompt for a password on the command line. The username value is obtained from the following locations, in order of higher precedence to lower precedence:\nThe --username global command-line option. The username option in the api_gateway section of the config file at ~/.config/sat/sat.toml. The name of currently logged in user running the sat command. If credentials are entered correctly when prompted by sat auth, a token file will be obtained and saved to ~/.config/sat/tokens. Subsequent sat commands will determine the username the same way as sat auth described above, and will use the token for that username if it has been obtained and saved by sat auth.\nPrerequisites The sat CLI has been installed following Install The System Admin Toolkit Product Stream. Procedure The following is the procedure to globally configure the username used by SAT and authenticate to the API gateway:\nGenerate a default SAT configuration file, if one does not exist.\nncn-m001# sat init Configuration file \u0026#34;/root/.config/sat/sat.toml\u0026#34; generated. Note: If the config file already exists, it will print out an error:\nERROR: Configuration file \u0026#34;/root/.config/sat/sat.toml\u0026#34; already exists. Not generating configuration file. Edit ~/.config/sat/sat.toml and set the username option in the api_gateway section of the config file. E.g.:\nusername = \u0026#34;crayadmin\u0026#34; Run sat auth. Enter your password when prompted. E.g.:\nncn-m001# sat auth Password for crayadmin: Succeeded! Other sat commands are now authenticated to make requests to the API gateway. E.g.:\nncn-m001# sat status Generate SAT S3 Credentials Generate S3 credentials and write them to a local file so the SAT user can access S3 storage. In order to use the SAT S3 bucket, the System Administrator must generate the S3 access key and secret keys and write them to a local file. This must be done on every Kubernetes master node where SAT commands are run.\nSAT uses S3 storage for several purposes, most importantly to store the site-specific information set with sat setrev (see: Run Sat Setrev to Set System Information).\nNOTE: This procedure is only required after initially installing SAT. It is not required after upgrading SAT.\nPrerequisites The sat CLI has been installed following Install The System Admin Toolkit Product Stream. The sat configuration file has been created (See SAT Authentication). CSM has been installed and verified. Procedure Ensure the files are readable only by root.\nncn-m001# touch /root/.config/sat/s3_access_key \\ /root/.config/sat/s3_secret_key ncn-m001# chmod 600 /root/.config/sat/s3_access_key \\ /root/.config/sat/s3_secret_key Write the credentials to local files using kubectl.\nncn-m001# kubectl get secret sat-s3-credentials -o json -o \\ jsonpath=\u0026#39;{.data.access_key}\u0026#39; | base64 -d \u0026gt; \\ /root/.config/sat/s3_access_key ncn-m001# kubectl get secret sat-s3-credentials -o json -o \\ jsonpath=\u0026#39;{.data.secret_key}\u0026#39; | base64 -d \u0026gt; \\ /root/.config/sat/s3_secret_key Verify the S3 endpoint specified in the SAT configuration file is correct.\nGet the SAT configuration file\u0026rsquo;s endpoint valie.\nNOTE: If the command\u0026rsquo;s output is commented out, indicated by an initial # character, the SAT configuration will take the default value â€“ \u0026quot;https://rgw-vip.nmn\u0026quot;.\nncn-m001# grep endpoint ~/.config/sat/sat.toml # endpoint = \u0026#34;https://rgw-vip.nmn\u0026#34; Get the sat-s3-credentials secret\u0026rsquo;s endpoint value.\nncn-m001# kubectl get secret sat-s3-credentials -o json -o \\ jsonpath=\u0026#39;{.data.s3_endpoint}\u0026#39; | base64 -d | xargs https://rgw-vip.nmn Compare the two endpoint values.\nIf the values differ, modify the SAT configuration file\u0026rsquo;s endpoint value to match the secret\u0026rsquo;s.\nCopy SAT configurations to every manager node on the system.\nncn-m001# for i in ncn-m002 ncn-m003; do echo $i; ssh ${i} \\ mkdir -p /root/.config/sat; \\ scp -pr /root/.config/sat ${i}:/root/.config; done NOTE: Depending on how many manager nodes are on the system, the list of manager nodes may be different. This example assumes three manager nodes, where the configuration files must be copied from ncn-m001 to ncn-m002 and ncn-m003. Therefore, the list of hosts above is ncn-m002 and ncn-m003.\nRun sat setrev to Set System Information NOTE: This procedure is only required after initially installing SAT. It is not required after upgrading SAT.\nPrerequisites S3 credentials have been generated. See Generate SAT S3 Credentials. SAT authentication has been set up. See SAT Authentication. Procedure Run sat setrev to set System Revision Information. Follow the on-screen prompts.\nncn-m001# sat setrev -------------------------------------------------------------------------------- Setting: Serial number Purpose: System identification. This will affect how snapshots are identified in the HPE backend services. Description: This is the top-level serial number which uniquely identifies the system. It can be requested from an HPE representative. Valid values: Alpha-numeric string, 4 - 20 characters. Type: \u0026lt;class \u0026#39;str\u0026#39;\u0026gt; Default: None Current value: None -------------------------------------------------------------------------------- Please do one of the following to set the value of the above setting: - Input a new value - Press CTRL-C to exit ... Run sat showrev to verify System Revision Information. The following tables contain example information.\nncn-m001# sat showrev ################################################################################ System Revision Information ################################################################################ +---------------------+---------------+ | component | data | +---------------------+---------------+ | Company name | HPE | | Country code | US | | Interconnect | Sling | | Product number | R4K98A | | Serial number | 12345 | | Site name | HPE | | Slurm version | slurm 20.02.5 | | System description | Test System | | System install date | 2021-01-29 | | System name | eniac | | System type | Shasta | +---------------------+---------------+ ################################################################################ Product Revision Information ################################################################################ +--------------+-----------------+------------------------------+------------------------------+ | product_name | product_version | images | image_recipes | +--------------+-----------------+------------------------------+------------------------------+ | csm | 0.8.14 | cray-shasta-csm-sles15sp1... | cray-shasta-csm-sles15sp1... | | sat | 2.0.1 | - | - | | sdu | 1.0.8 | - | - | | slingshot | 0.8.0 | - | - | | sma | 1.4.12 | - | - | +--------------+-----------------+------------------------------+------------------------------+ ################################################################################ Local Host Operating System ################################################################################ +-----------+----------------------+ | component | version | +-----------+----------------------+ | Kernel | 5.3.18-24.15-default | | SLES | SLES 15-SP2 | +-----------+----------------------+ Optional: Remove old versions after an upgrade Prerequisites The Install the System Admin Toolkit Product Stream procedure has been successfully completed. The Perform NCN Personalization procedure has been successfully completed. Procedure After upgrading from a previous version of SAT, the old version of the cray/cray-sat container image will remain in the registry on the system. It is not removed automatically, but it will not be the default version.\nThe admin can remove the older version of the cray/cray-sat container image.\nThe cray-product-catalog Kubernetes configuration map will also show all versions of SAT that are installed. The command sat showrev --products will display these versions. See the example:\nncn-m001# sat showrev --products ############################################################################### Product Revision Information ############################################################################### +--------------+-----------------+--------------------+-----------------------+ | product_name | product_version | images | image_recipes | +--------------+-----------------+--------------------+-----------------------+ ... | sat | 2.1.3 | - | - | | sat | 2.0.4 | - | - | ... +--------------+-----------------+--------------------+-----------------------+ Remove obsolete configuration file sections Prerequisites The Install the System Admin Toolkit Product Stream procedure has been successfully completed. The Perform NCN Personalization procedure has been successfully completed. Procedure After upgrading SAT, if using the configuration file from a previous version, there may be configuration file sections no longer used in the new version. For example, when upgrading from Shasta 1.4 to Shasta 1.5, the [redfish] configuration file section is no longer used. In that case, the following warning may appear upon running sat commands.\nWARNING: Ignoring unknown section \u0026#39;redfish\u0026#39; in config file. Remove the [redfish] section from /root/.config/sat/sat.toml to resolve the warning.\n[redfish] username = \u0026#34;admin\u0026#34; password = \u0026#34;adminpass\u0026#34; Repeat this process for any configuration file sections for which there are \u0026ldquo;unknown section\u0026rdquo; warnings.\n"
},
{
	"uri": "/docs-sat/en-21/introduction/",
	"title": "Introduction to SAT",
	"tags": [],
	"description": "",
	"content": "Introduction to SAT About System Admin Toolkit (SAT) The System Admin Toolkit (SAT) is designed to assist administrators with common tasks, such as troubleshooting and querying information about the HPE Cray EX System and its components, system boot and shutdown, and replacing hardware components.\nSAT offers a command line utility which uses subcommands. There are similarities between SAT commands and xt commands used on the Cray XC platform. For more information on SAT commands, see System Admin Toolkit Command Overview.\nSix Kibana Dashboards are included with SAT. They provide organized output for system health information.\nAER Kibana Dashboard ATOM Kibana Dashboard Heartbeat Kibana Dashboard Kernel Kibana Dashboard MCE Kibana Dashboard Rasdaemon Kibana Dashboard Four Grafana Dashboards are included with SAT. They display messages that are generated by the HSN (High Speed Network) and are reported through Redfish.\nGrafana Fabric Congestion Dashboard Grafana Fabric Errors Dashboard Grafana Fabric Port State Dashboard Grafana Fabric RFC3635 Dashboard SAT is installed as a separate product as part of the HPE Cray EX System base installation.\nSystem Admin Toolkit Command Overview Describes the SAT Command Line Utility, lists the key commands found in the System Admin Toolkit man pages, and provides instruction on the SAT Container Environment.\nSAT Command Line Utility The primary component of the System Admin Toolkit (SAT) is a command-line utility run from Kubernetes manager nodes (ncn-m nodes).\nIt is designed to assist administrators with common tasks, such as troubleshooting and querying information about the HPE Cray EX System and its components, system boot and shutdown, and replacing hardware components. There are similarities between SAT commands and xt commands used on the Cray XC platform.\nSAT Commands The top-level SAT man page describes the toolkit, documents the global options affecting all subcommands, documents configuration file options, and references the man page for each subcommand. SAT consists of many subcommands that each have their own set of options.\nSAT Container Environment The sat command-line utility runs in a container using podman, a daemonless container runtime. SAT runs on Kubernetes manager nodes. A few important points about the SAT container environment include the following:\nUsing either sat or sat bash always launches a container. The SAT container does not have access to the NCN file system. There are two ways to run sat.\nInteractive: Launching a container using sat bash, followed by a sat command. Non-interactive: Running a sat command directly on a Kubernetes manager node. In both of these cases, a container is launched in the background to execute the command. The first option, running sat bash first, gives an interactive shell, at which point sat commands can be run. In the second option, the container is launched, executes the command, and upon the command\u0026rsquo;s completion the container exits. The following two examples show the same action, checking the system status, using interactive and non-interactive modes.\nInteractive ncn-m001# sat bash (CONTAINER-ID)sat-container# sat status Non-interactive ncn-m001# sat status Interactive Advantages Running sat using the interactive command prompt gives the ability to read and write local files on ephemeral container storage. If multiple sat commands are being run in succession, then use sat bash to launch the container beforehand. This will save time because the container does not need to be launched for each sat command.\nNon-interactive Advantages The non-interactive mode is useful if calling sat with a script, or when running a single sat command as a part of several steps that need to be executed from a management NCN.\nMan Pages - Interactive and Non-interactive Modes To view a sat man page from a Kubernetes manager node, use sat-man on the manager node as shown in the following example.\nncn-m001# sat-man status A man page describing the SAT container environment is available on the Kubernetes manager nodes, which can be viewed either with man sat or man sat-podman from the manager node.\nncn-m001# man sat ncn-m001# man sat-podman Command Prompt Conventions in SAT The host name in a command prompt indicates where the command must be run. The account that must run the command is also indicated in the prompt.\nThe root or super-user account always has the # character at the end of the prompt and has the host name of the host in the prompt. Any non-root account is indicated with account@hostname\u0026gt;. A user account that is neither root nor crayadm is referred to as user. The command prompt inside the SAT container environment is indicated with the string as follows. It also has the \u0026ldquo;#\u0026rdquo; character at the end of the prompt. Command Prompt Meaning ncn-m001# Run on one of the Kubernetes Manager servers. (Non-interactive) (CONTAINER_ID) sat-container# Run the command inside the SAT container environment by first running sat bash. (Interactive) Examples of the sat status command used by an administrator:\nncn-m001# sat status ncn-m001# sat bash (CONTAINER_ID) sat-container# sat status SAT Dependencies Most sat subcommands depend on services or components from other products in the HPE Cray EX (Shasta) software stack. The following list shows these dependencies for each subcommand. Each service or component is listed under the product it belongs to.\nsat auth CSM Keycloak sat bootsys CSM Boot Orchestration Service (BOS) Cray Advanced Platform Monitoring and Control (CAPMC) Ceph Compute Rolling Upgrade Service (CRUS) Etcd Firmware Action Service (FAS) Hardware State Manager (HSM) Kubernetes S3 COS Node Memory Dump (NMD) sat diag CSM Hardware State Manager (HSM) CSM-Diag Fox sat firmware CSM Firmware Action Service (FAS) sat hwinv CSM Hardware State Manager (HSM) sat hwmatch CSM Hardware State Manager (HSM) sat init None\nsat k8s CSM Kubernetes sat nid2xname CSM Hardware State Manager (HSM) sat sensors CSM Hardware State Manager (HSM) HM Collector SMA Telemetry API sat setrev CSM S3 sat showrev CSM Hardware State Manager (HSM) Kubernetes S3 sat status CSM Hardware State Manager (HSM) sat swap Slingshot Fabric Manager sat switch Deprecated: See sat swap\nsat xname2nid CSM Hardware State Manager (HSM) "
},
{
	"uri": "/docs-sat/en-21/dashboards/sat_grafana_dashboards/",
	"title": "SAT Grafana Dashboards",
	"tags": [],
	"description": "",
	"content": "SAT Grafana Dashboards The SAT Grafana Dashboards display messages that are generated by the HSN (High Speed Network) and reported through Redfish. The messages are displayed based on severity.\nGrafana can be accessed via web browser at the following URL:\nhttps://sma-grafana.\u0026lt;system_name\u0026gt;.\u0026lt;system_domain\u0026gt; For additional details about how to access the Grafana Dashboards refer to Access the Grafana Monitoring UI in the SMA product documentation.\nFor more information about the interpretation of metrics for the SAT Grafana Dashboards refer to Fabric Telemetry Kafka Topics in the SMA product documentation.\nNavigate SAT Grafana Dashboards There are four Fabric Telemetry dashboards used in SAT that report on the HSN. Two contain chart panels and two display telemetry in a tabular format.\nDashboard Name Display Type Fabric Congestion Chart Panels Fabric RFC3635 Chart Panels Fabric Errors Tabular Format Fabric Port State Tabular Format The tabular format presents a single point of telemetry for a given location and metric, either because the telemetry is not numerical or that it changes infrequently. The value shown is the most recently reported value for that location during the time range selected, if any. The interval setting is not used for tabular dashboards.\nSAT Grafana Interval and Locations Options Shows the Interval and Locations Options for the available telemetry.\nThe value of the Interval option sets the time resolution of the received telemetry. This works a bit like a histogram, with the available telemetry in an interval of time going into a \u0026ldquo;bucket\u0026rdquo; and averaging out to a single point on the chart or table. The special value auto will choose an interval based on the time range selected.\nFor additional information, refer to Grafana Templates and Variables.\nThe Locations option allows restriction of the telemetry shown by locations, either individual links or all links in a switch. The selection presented updates dynamically according to time range, except for the errors dashboard, which always has entries for all links and switches, although the errors shown are restricted to the selected time range.\nThe chart panels for the RFC3635 and Congestion dashboards allow selection of a single location from the chart\u0026rsquo;s legend or the trace on the chart.\nGrafana Fabric Congestion Dashboard SAT Grafana Dashboards provide system administrators a way to view fabric telemetry data across all Rosetta switches in the system and assess the past and present health of the high-speed network. It also allows the ability to drill down to view data for specific ports on specific switches.\nThis dashboard contains the variable, Port Type not found in the other dashboards. The possible values are edge, local, and global and correspond to the link\u0026rsquo;s relationship to the network topology. The locations presented in the panels are restricted to the values (any combination, defaults to \u0026ldquo;all\u0026rdquo;) selected.\nThe metric values for links of a given port type are similar in value to each other but very distinct from the values of other types. If the values for different port types are all plotted together, the values for links with lower values are indistinguishable from zero when plotted.\nThe port type of a link is reported as a port state \u0026ldquo;subtype\u0026rdquo; event when defined at port initialization.\nGrafana Fabric Errors Dashboard This dashboard reports error counters in a tabular format in three panels.\nThere is no Interval option because this parameter is not used to set a coarseness of the data. Only a single value is presented that displays the most recent value in the time range.\nUnlike other dashboards, the locations presented are all locations in the system rather than having telemetry within the time range selected. However, the values are taken from telemetry within the time range.\nGrafana Fabric Port State Dashboard There is no Interval option because this parameter is not used to set a coarseness of the data. Only a single value is presented that displays the most recent value in the time range.\nThe Fabric Port State telemetry is distinct because it typically is not numeric. It also updates infrequently, so a long time range may be necessary to obtain any values. Port State is refreshed daily, so a time range of 24 hours results in all states for all links in the system being shown.\nThe three columns named, group, switch, and port are not port state events, but extra information included with all port state events.\nGrafana Fabric RFC3635 Dashboard For additional information on performance counters, refer to Definitions of Managed Objects for the Ethernet-like Interface Types, an Internet standards document.\nBecause these metrics are counters that only increase over time, the values plotted are the change in the counter\u0026rsquo;s value over the interval setting.\n"
},
{
	"uri": "/docs-sat/en-21/dashboards/sat_kibana_dashboards/",
	"title": "SAT Kibana Dashboards",
	"tags": [],
	"description": "",
	"content": "SAT Kibana Dashboards Kibana is an open source analytics and visualization platform designed to search, view, and interact with data stored in Elasticsearch indices. Kibana runs as a web service and has a browser-based interface. It offers visual output of node data in the forms of charts, tables and maps that display real-time Elasticsearch queries. Viewing system data in this way breaks down the complexity of large data volumes into easily understood information.\nKibana can be accessed via web browser at the following URL:\nhttps://sma-kibana.\u0026lt;system_name\u0026gt;.\u0026lt;system_domain\u0026gt; For additional details about how to access the Kibana Dashboards refer to View Logs Via Kibana in the SMA product documentation.\nAdditional details about the AER, ATOM, Heartbeat, Kernel, MCE, and Rasdaemon Kibana Dashboards are included in this table.\nDashboard Short Description Long Description Kibana Visualization and Search Name sat-aer AER corrected Corrected Advanced Error Reporting messages from PCI Express devices on each node. Visualization: aer-corrected Search: sat-aer-corrected sat-aer AER fatal Fatal Advanced Error Reporting messages from PCI Express devices on each node. Visualization: aer-fatal Search: sat-aer-fatal sat-atom ATOM failures Application Task Orchestration and Management tests are run on a node when a job finishes. Test failures are logged. sat-atom-failed sat-atom ATOM admindown Application Task Orchestration and Management test failures can result in nodes being marked admindown. An admindown node is not available for job launch. sat-atom-admindown sat-heartbeat Heartbeat loss events Heartbeat loss event messages reported by the hbtd pods that monitor for heartbeats across nodes in the system. sat-heartbeat sat-kernel Kernel assertions The kernel software performs a failed assertion when some condition represents a serious fault. The node goes down. sat-kassertions sat-kernel Kernel panics The kernel panics when something is seriously wrong. The node goes down. sat-kernel-panic sat-kernel Lustre bugs (LBUGs) The Lustre software in the kernel stack performs a failed assertion when some condition related to file system logic represents a serious fault. The node goes down. sat-lbug sat-kernel CPU stalls CPU stalls are serous conditions that can reduce node performance, and sometimes cause a node to go down. Technically these are Read-Copy-Update stalls where software in the kernel stack holds onto memory for too long. Read-Copy-Update is a vital aspect of kernel performance and rather esoteric. sat-cpu-stall sat-kernel Out of memory An Out Of Memory (OOM) condition has occurred. The kernel must kill a process to continue. The kernel will select an expendable process when possible. If there is no expendable process the node usually goes down in some manner. Even if there are expendable processes the job is likely to be impacted. OOM conditions are best avoided. sat-oom sat-mce MCE Machine Check Exceptions (MCE) are errors detected at the processor level. sat-mce sat-rasdaemon rasdaemon errors Errors from the rasdaemon service on nodes. The rasdaemon service is the Reliability, Availability, and Serviceability Daemon, and it is intended to collect all hardware error events reported by the linux kernel, including PCI and MCE errors. This may include certain HSN errors in the future. sat-rasdaemon-error sat-rasdaemon rasdaemon messages All messages from the rasdaemon service on nodes. sat-rasdaemon Disable Search Highlighting in Kibana Dashboard By default, search highlighting is enabled. This procedure instructs how to disable search highlighting.\nThe Kibana Dashboard should be open on your system.\nNavigate to Management\nNavigate to Advanced Settings in the Kibana section, below the Elastic search section\nScroll down to the Discover section\nChange Highlight results from on to off\nClick Save to save changes\nAER Kibana Dashboard The AER Dashboard displays errors that come from the PCI Express Advanced Error Reporting (AER) driver. These errors are split up into separate visualizations depending on whether they are fatal or corrected errors.\nView the AER Kibana Dashboard Go to the dashboard section.\nSelect sat-aer dashboard.\nChoose the time range of interest.\nView the Corrected and Fatal Advanced Error Reporting messages from PCI Express devices on each node. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\nATOM Kibana Dashboard The ATOM (Application Task Orchestration and Management) Dashboard displays node failures that occur during health checks and application test failures. Some test failures are of possible interest even though a node is not marked admindown or otherwise fails. They are of clear interest if a node is marked admindown, and might provide clues if a node otherwise fails. They might also show application problems.\nView the ATOM Kibana Dashboard HPE Cray EX is installed on the system along with the System Admin Toolkit, which contains the ATOM Kibana Dashboard.\nGo to the dashboard section.\nSelect sat-atom dashboard.\nChoose the time range of interest.\nView any nodes marked admindown and any ATOM test failures. These failures occur during health checks and application test failures. Test failures marked admindown are important to note. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\nHeartbeat Kibana Dashboard The Heartbeat Dashboard displays heartbeat loss messages that are logged by the hbtd pods in the system. The hbtd pods are responsible for monitoring nodes in the system for heartbeat loss.\nView the Heartbeat Kibana Dashboard Go to the dashboard section.\nSelect sat-heartbeat dashboard.\nChoose the time range of interest.\nView the heartbeat loss messages that are logged by the hbtd pods in the system. The hbtd pods are responsible for monitoring nodes in the system for heartbeat loss.View the matching log messages in the panel.\nKernel Kibana Dashboard The Kernel Dashboard displays compute node failures such as kernel assertions, kernel panics, and Lustre LBUG messages. The messages reveal if Lustre has experienced a fatal error on any compute nodes in the system. A CPU stall is a serious problem that might result in a node failure. Out-of-memory conditions can be due to applications or system problems and may require expert analysis. They provide useful clues for some node failures and may reveal if an application is using too much memory.\nView the Kernel Kibana Dashboard Go to the dashboard section.\nSelect sat-kernel dashboard.\nChoose the time range of interest.\nView the compute node failures such as kernel assertions, kernel panics, and Lustre LBUG messages. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\nMCE Kibana Dashboard The MCE Dashboard displays CPU detected processor-level hardware errors.\nView the MCE Kibana Dashboard Go to the dashboard section.\nSelect sat-mce dashboard.\nChoose the time range of interest.\nView the Machine Check Exceptions (MCEs) listed including the counts per NID (node). For an MCE, the CPU number and DIMM number can be found in the message, if applicable. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\nRasdaemon Kibana Dashboard The Rasdaemon Dashboard displays errors that come from the Reliability, Availability, and Serviceability (RAS) daemon service on nodes in the system. This service collects all hardware error events reported by the linux kernel, including PCI and MCE errors. As a result there may be some duplication between the messages presented here and the messages presented in the MCE and AER dashboards. This dashboard splits up the messages into two separate visualizations, one for only messages of severity \u0026ldquo;emerg\u0026rdquo; or \u0026ldquo;err\u0026rdquo; and another for all messages from rasdaemon.\nView the Rasdaemon Kibana Dashboard Go to the dashboard section.\nSelect sat-rasdaemon dashboard.\nChoose the time range of interest.\nView the errors that come from the Reliability, Availability, and Serviceability (RAS) daemon service on nodes in the system. View the matching log messages in the panel(s) on the right, and view the counts of each message per NID in the panel(s) on the left. If desired, results can be filtered by NID by clicking the icon showing a + inside a magnifying glass next to each NID.\n"
},
{
	"uri": "/docs-sat/en-21/release_notes/",
	"title": "SAT Release Notes",
	"tags": [],
	"description": "",
	"content": "SAT Release Notes Summary of SAT changes in Shasta v1.5 We released version 2.1.16 of the SAT product in Shasta v1.5.\nThis version of the SAT product included:\nVersion 3.7.4 of the sat python package and CLI Version 1.4.10 of the sat-podman wrapper script It also added the following new component:\nVersion 1.0.3 of the sat-cfs-install docker image and helm chart The following sections detail the changes in this release.\nInstall Changes to Separate Product from CSM This release further decouples the installation of the SAT product from the CSM product. The cray-sat-podman RPM is no longer installed in the management non-compute node (NCN) image. Instead, the cray-sat-podman RPM is installed on all master management NCNs via an Ansible playbook which is referenced by a layer of the CFS configuration that applies to management NCNs. This CFS configuration is typically named \u0026ldquo;ncn-personalization\u0026rdquo;.\nThe SAT product now includes a Docker image and a Helm chart named sat-cfs-install. The SAT install script, install.sh, deploys the Helm chart with Loftsman. This helm chart deploys a Kubernetes job that imports the SAT Ansible content to a git repository in VCS (Gitea) named sat-config-management. This repository is referenced by the layer added to the NCN personalization CFS configuration.\nRemoval of Direct Redfish Access All commands which used to access Redfish directly have either been removed or modified to use higher-level service APIs. This includes the following commands:\nsat sensors sat diag sat linkhealth The sat sensors command has been rewritten to use the SMA telemetry API to obtain the latest sensor values. The command\u0026rsquo;s usage has changed slightly, but legacy options work as before, so it is backwards compatible. Additionally, new commands have been added.\nThe sat diag command has been rewritten to use a new service called Fox, which is delivered with the CSM-diags product. The sat diag command now launches diagnostics using the Fox service, which launches the corresponding diagnostic executables on controllers using the Hardware Management Job and Task Daemon (HMJTD) over Redfish. Essentially, Fox serves as a proxy for us to start diagnostics over Redfish.\nThe sat linkhealth command has been removed. Its functionality has been replaced by functionality from the Slingshot Topology Tool (STT) in the fabric manager pod.\nThe Redfish username and password command line options and config file options have been removed. For further instructions, see Remove Obsolete Configuration File Sections.\nAdditional Fields in sat setrev and sat showrev sat setrev now collects the following information from the admin, which is then displayed by sat showrev:\nSystem description Product number Company name Country code Additional guidance and validation has been added to each field collected by sat setrev. This sets the stage for sdu setup to stop collecting this information and instead collect it from sat showrev or its S3 bucket.\nImprovements to sat bootsys The platform-services stage of the sat bootsys boot command has been improved to start inactive Ceph services, unfreeze Ceph, and wait for Ceph health in the correct order. The ceph-check stage has been removed as it is no longer needed.\nThe platform-services stage of sat bootsys boot now prompts for confirmation of the storage NCN hostnames in addition to the Kubernetes masters and workers.\nBug Fixes and Security Fixes Improved error handling in sat firmware. Incremented version of Alpine Linux to 3.13.2 to address a security vulnerability. Other Notable Changes Ansible has been removed from the cray-sat container image. Support for the Firmware Update Service (FUS) has been removed from the sat firmware command. Summary of SAT Changes in Shasta v1.4.1 We released version 2.0.4 of the SAT product in Shasta v1.4.1.\nThis version of the SAT product included:\nVersion 3.5.0 of the sat python package and CLI. Version 1.4.3 of the sat-podman wrapper script. The following sections detail the changes in this release.\nNew Commands to Translate Between NIDs and XNames Two new commands were added to translate between NIDs and XNames:\nsat nid2xname sat xname2nid These commands perform this translation by making requests to the Hardware State Manager (HSM) API.\nBug Fixes Fixed a problem in sat swap where creating the offline port policy failed. Changed sat bootsys shutdown --stage bos-operations to no longer forcefully power off all compute nodes and application nodes using CAPMC when BOS sessions complete or time out. Fixed an issue with the command sat bootsys boot --stage cabinet-power. Summary of SAT Changes in Shasta v1.4 In Shasta v1.4, SAT became an independent product, which meant we began to designate a version number for the entire SAT product. We released version 2.0.3 of the SAT product in Shasta v1.4.\nThis version of the SAT product included the following components:\nVersion 3.4.0 of the sat python package and CLI It also added the following new component:\nVersion 1.4.2 of the sat-podman wrapper script The following sections detail the changes in this release.\nSAT as an Independent Product SAT is now packaged and released as an independent product. The product deliverable is called a \u0026ldquo;release distribution\u0026rdquo;. The release distribution is a gzipped tar file containing an install script. This install script loads the cray/cray-sat container image into the Docker registry in Nexus and loads the cray-sat-podman RPM into a package repository in Nexus.\nIn this release, the cray-sat-podman package is still installed in the master and worker NCN images provided by CSM. This is changed in SAT 2.1.16 released in Shasta v1.5.\nSAT Running in a Container Under Podman The sat command now runs in a container under Podman. The sat executable is now installed on all nodes in the Kubernetes management cluster (i.e., workers and masters). This executable is a wrapper script that starts a SAT container in Podman and invokes the sat Python CLI within that container. The admin can run individual sat commands directly on the master or worker NCNs as before, or they can run sat commands inside the SAT container after using sat bash to enter an interactive shell inside the SAT container.\nTo view man pages for sat commands, the user can run sat-man SAT_COMMAND, replacing SAT_COMMAND with the name of the sat command. Alternatively, the user can enter the sat container with sat bash and use the man command.\nNew sat init Command and Config File Location Change The default location of the SAT config file has been changed from /etc/sat.toml to ~/.config/sat/sat.toml. A new command, sat init, has been added that initializes a configuration file in the new default directory. This better supports individual users on the system who want their own config files.\n~/.config/sat is mounted into the container that runs under Podman, so changes are persistent across invocations of the sat container. If desired, an alternate configuration directory can be specified with the SAT_CONFIG_DIR environment variable.\nAdditionally, if a config file does not yet exist when a user runs a sat command, one is generated automatically.\nAdditional Types Added to sat hwinv Additional functionality has been added to sat hwinv including:\nList node enclosure power supplies with the --list-node-enclosure-power-supplies option. List node accelerators (e.g., GPUs) with the --list-node-accels option. The count of node accelerators is also included for each node. List node accelerator risers (e.g., Redstone modules) with the --list-node-accel-risers option. The count of node accelerator risers is also included for each node. List High-Speed Node Network Interface Cards (HSN NICs) with the --list-node-hsn-nics option. The count of HSN NICs is also included for each node. Documentation for these new options has been added to the man page for sat hwinv.\nSite Information Stored by sat setrev in S3 The sat setrev and sat showrev commands now use S3 to store and obtain site information, including system name, site name, serial number, install date, and system type. Since the information is stored in S3, it will now be consistent regardless of the node on which sat is executed.\nAs a result of this change, S3 credentials must be configured for SAT. For detailed instructions, see Generate SAT S3 Credentials.\nProduct Version Information Shown by sat showrev sat showrev now shows product information from the cray-product-catalog ConfigMap in Kubernetes.\nAdditional Changes to sat showrev The output from sat showrev has also been changed in the following ways:\nThe --docker and --packages options were considered misleading and have been removed. Information pertaining to only to the local host, where the command is run, has been moved to the output of the --local option. Removal of sat cablecheck The sat cablecheck command has been removed. To verify that the system\u0026rsquo;s Slingshot network is cabled correctly, admins should now use the show cables command in the Slingshot Topology Tool (STT).\nsat swap Command Compatibility with Next-gen Fabric Controller The sat swap command was added in Shasta v1.3.2. This command used the Fabric Controller API. Shasta v1.4 introduced a new Fabric Manager API and removed the Fabric Controller API, so this command has been rewritten to use the new backwards-incompatible API. Usage of the command did not change.\nsat bootsys Functionality Much of the functionality added to sat bootsys in Shasta v1.3.2 was broken by changes introduced in Shasta v1.4, which removed the Ansible inventory and playbooks.\nThe functionality in the platform-services stage of sat bootsys has been re-implemented to use python directly instead of Ansible. This resulted in a more robust procedure with better logging to the sat log file. Failures to stop containers on Kubernetes nodes are handled more gracefully, and more information about the containers that failed to stop, including how to debug the problem, is included.\nImprovements were made to console logging setup for non-compute nodes (NCNs) when they are shut down and booted.\nThe following improvements were made to the bos-operations stage of sat bootsys:\nMore information about the BOS sessions, BOA jobs, and BOA pods is printed. A command-line option, --bos-templates, and a corresponding config-file option, bos_templates, were added, and the --cle-bos-template and --uan-bos-template options and their corresponding config file options were deprecated. The following functionality has been removed from sat bootsys:\nThe hsn-bringup stage of sat bootsys boot has been removed due to removal of the underlying Ansible playbook. The bgp-check stage of sat bootys {boot,shutdown} has been removed. It is now a manual procedure. Log File Location Change The location of the sat log file has changed from /var/log/cray/sat.log to /var/log/cray/sat/sat.log. This change simplifies mounting this file into the sat container running under Podman.\nSummary of SAT Changes in Shasta v1.3.2 Shasta v1.3.2 included version 2.4.0 of the sat python package and CLI.\nThe following sections detail the changes in this release.\nsat swap Command for Switch and Cable Replacement The sat switch command which supported operations for replacing a switch has been deprecated and replaced with the sat swap command, which now supports replacing a switch OR cable.\nThe sat swap switch command is equivalent to sat switch. The sat switch command will be removed in a future release.\nAddition of Stages to sat bootsys Command The sat bootsys command now has multiple stages for both the boot and shutdown actions. Please refer to the \u0026ldquo;System Power On Procedures\u0026rdquo; and \u0026ldquo;System Power Off Procedures\u0026rdquo; sections of the Cray Shasta Administration Guide (S-8001) for more details on using this command in the context of a full system power off and power on.\nSummary of SAT Changes in Shasta v1.3 Shasta v1.3 included version 2.2.3 of the sat python package and CLI.\nThis version of the sat CLI contained the following commands:\nauth bootsys cablecheck diag firmware hwinv hwmatch k8s linkhealth sensors setrev showrev status swap switch See the System Admin Toolkit Command Overview and the table of commands in the SAT Authentication section of this document for more details on each of these commands.\n"
},
{
	"uri": "/docs-sat/en-21/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "/docs-sat/en-21/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]